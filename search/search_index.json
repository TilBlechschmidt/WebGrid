{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 Here shall be a tag line and a link to GitHub plus a link to the Getting started","title":"Home"},{"location":"#home","text":"Here shall be a tag line and a link to GitHub plus a link to the Getting started","title":"Home"},{"location":"faq/","text":"FAQ \u00b6 Technical details \u00b6 Does it work with Selenium? \u00b6 Yes it does, thanks to the standardization of the underlying protocol by the W3C. How well does it scale \u00b6 Let's just say that we haven't reached any performance limits yet in our internal testing with a few hundred browser instances. In theory, the only limit is your cluster bandwidth and to some degree the performance of the Redis database server (although this only affects session creation and not running browser sessions). About this project \u00b6 Why does this exist? \u00b6 As of early 2020 only a handful solutions for distributed selenium based software tests existed. Most of these did not provide support for advanced features like dynamic scaling/provisioning and efficient video recordings with additional problems like thread safety issues and poor scaling for large scale applications with hundreds of browsers. Selenium Grid Selenium Grid has been in use at my current workplace for some years and worked acceptable. However, with projects growing in both test volume and manpower the amount of concurrent browsers grew rather quickly. This exposed known bottlenecks in the single-proxy design of the Selenium Grid's architecture (which yielded projects like GridRouter which try to work around the fundamental design problem). Additionally, it raised concerns about the constant resource usage due to the static allocation of nodes creating a requirement for dynamic allocation. Add to this the fact that Grid 3 has no official support anymore and Grid 4 has been in development for about two years now with no release date in sight! Zalenium Zalando ran into similar findings and created an extension to the regular Selenium Grid called Zalenium . It boasts features like a dashboard with VNC viewers and screen recordings. However, it had some its own fair share of issues on top of the ones that the regular grid exposed, yielding even worse test flakiness on a daily basis. Zalando stopped maintaining the project as of early 2020. Commercial solutions Aerokube provides a commercial off-the-shelve solution for scalable selenium grids in Kubernetes. However, for our application the pricing philosophy was out of reach by a long stretch and it was cheaper to set aside some development resources to create this Open Source solution with the added benefit of making scalable Grids available to the community! This lead to a investigation of our options to continue Selenium tests with dynamic scaling, screen recording and other future additions on the wish-list. As the underlying protocol of Selenium has been standardized by the W3C we reached the conclusion that it was feasible to develop our own solution to this problem within a few months time. With that this project was born in April 2020. Who is behind this? \u00b6 The project originates from an internal requirement at PPI AG . However, as the project progressed it became clear that other people could greatly benefit from it. I have personally taken over the public development and maintenance of the project on GitHub and it is no longer directly affiliated with the PPI AG nor does the company provide any kind of support or responsibility! I am a computer science student from Germany, currently working at PPI AG and studying Applied Computer Science at the Nordakademie in my last semester. Why are there so few issues in the tracker? \u00b6 The project has been developed internally at first using a private GitLab instance. Later, the decision to go public has been made and everything except past issues has been moved over. Where is the latest tag? \u00b6 We believe that the :latest in Docker is very evil and dangerous . Apart from the problem that it is just yet another tag and everything else being purely convention, you should make a conscious choice to upgrade your production environment from one version to another. For this reason we are releasing versions on all distribution platforms following the SemVer 2.0 convention so you can know which versions are safe and which might require some more work. Additionally, all Helm charts and Compose files use pinned versions for the Docker images.","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#technical-details","text":"","title":"Technical details"},{"location":"faq/#does-it-work-with-selenium","text":"Yes it does, thanks to the standardization of the underlying protocol by the W3C.","title":"Does it work with Selenium?"},{"location":"faq/#how-well-does-it-scale","text":"Let's just say that we haven't reached any performance limits yet in our internal testing with a few hundred browser instances. In theory, the only limit is your cluster bandwidth and to some degree the performance of the Redis database server (although this only affects session creation and not running browser sessions).","title":"How well does it scale"},{"location":"faq/#about-this-project","text":"","title":"About this project"},{"location":"faq/#why-does-this-exist","text":"As of early 2020 only a handful solutions for distributed selenium based software tests existed. Most of these did not provide support for advanced features like dynamic scaling/provisioning and efficient video recordings with additional problems like thread safety issues and poor scaling for large scale applications with hundreds of browsers. Selenium Grid Selenium Grid has been in use at my current workplace for some years and worked acceptable. However, with projects growing in both test volume and manpower the amount of concurrent browsers grew rather quickly. This exposed known bottlenecks in the single-proxy design of the Selenium Grid's architecture (which yielded projects like GridRouter which try to work around the fundamental design problem). Additionally, it raised concerns about the constant resource usage due to the static allocation of nodes creating a requirement for dynamic allocation. Add to this the fact that Grid 3 has no official support anymore and Grid 4 has been in development for about two years now with no release date in sight! Zalenium Zalando ran into similar findings and created an extension to the regular Selenium Grid called Zalenium . It boasts features like a dashboard with VNC viewers and screen recordings. However, it had some its own fair share of issues on top of the ones that the regular grid exposed, yielding even worse test flakiness on a daily basis. Zalando stopped maintaining the project as of early 2020. Commercial solutions Aerokube provides a commercial off-the-shelve solution for scalable selenium grids in Kubernetes. However, for our application the pricing philosophy was out of reach by a long stretch and it was cheaper to set aside some development resources to create this Open Source solution with the added benefit of making scalable Grids available to the community! This lead to a investigation of our options to continue Selenium tests with dynamic scaling, screen recording and other future additions on the wish-list. As the underlying protocol of Selenium has been standardized by the W3C we reached the conclusion that it was feasible to develop our own solution to this problem within a few months time. With that this project was born in April 2020.","title":"Why does this exist?"},{"location":"faq/#who-is-behind-this","text":"The project originates from an internal requirement at PPI AG . However, as the project progressed it became clear that other people could greatly benefit from it. I have personally taken over the public development and maintenance of the project on GitHub and it is no longer directly affiliated with the PPI AG nor does the company provide any kind of support or responsibility! I am a computer science student from Germany, currently working at PPI AG and studying Applied Computer Science at the Nordakademie in my last semester.","title":"Who is behind this?"},{"location":"faq/#why-are-there-so-few-issues-in-the-tracker","text":"The project has been developed internally at first using a private GitLab instance. Later, the decision to go public has been made and everything except past issues has been moved over.","title":"Why are there so few issues in the tracker?"},{"location":"faq/#where-is-the-latest-tag","text":"We believe that the :latest in Docker is very evil and dangerous . Apart from the problem that it is just yet another tag and everything else being purely convention, you should make a conscious choice to upgrade your production environment from one version to another. For this reason we are releasing versions on all distribution platforms following the SemVer 2.0 convention so you can know which versions are safe and which might require some more work. Additionally, all Helm charts and Compose files use pinned versions for the Docker images.","title":"Where is the latest tag?"},{"location":"getting-started/","text":"Getting started \u00b6 Below are guides to get you started as quickly as possible on your specific platform! They provide a set of sane defaults which can later be tweaked for your use-case. Docker \u00b6 If you want to run a grid locally for simple testing purposes that require multiple isolated browsers or just want to evaluate this tool this is the right choice. Make sure you have Docker installed and configured properly. Before you can start the grid in Docker you have to create a network and volume for it: docker volume create webgrid docker network create webgrid Next you need to download the latest docker-compose.yml and run it: # Download compose file curl -fsSLO https://webgrid.dev/docker-compose.yml # Launch the grid docker-compose up Continue reading below on how to send requests to your grid. Todo The compose file refers locally built images. Replace them with hub.docker.com ones once the repository goes public! Kubernetes \u00b6 WebGrid provides a Helm chart to get started as quickly as possible. Below is a guide on how to add the chart repository and install the chart. You can change the name of the release in the second command or add other options like the target namespace \u2014 for more details consult the Helm documentation. # Add the repository helm repo add webgrid https://webgrid.dev/ # Install the chart helm install example webgrid/webgrid Using the grid \u00b6 Once you have started the grid you can send requests to it using the regular Selenium client libraries available here . Java FirefoxOptions firefoxOptions = new FirefoxOptions (); WebDriver driver = new RemoteWebDriver ( new URL ( \"http://localhost\" ), firefoxOptions ); driver . get ( \"http://www.google.com\" ); driver . quit (); Python from selenium import webdriver firefox_options = webdriver . FirefoxOptions () driver = webdriver . Remote ( command_executor = 'http://localhost' , options = firefox_options ) driver . get ( \"http://www.google.com\" ) driver . quit () C# FirefoxOptions firefoxOptions = new FirefoxOptions (); IWebDriver driver = new RemoteWebDriver ( new Uri ( \"http://localhost\" ), firefoxOptions ); driver . Navigate (). GoToUrl ( \"http://www.google.com\" ); driver . Quit (); Ruby require 'selenium-webdriver' driver = Selenium :: WebDriver . for :remote , url : \"http://localhost\" , desired_capabilities : :firefox driver . get \"http://www.google.com\" driver . close JavaScript const { Builder , Capabilities } = require ( \"selenium-webdriver\" ); var capabilities = Capabilities . firefox (); ( async function helloSelenium () { let driver = new Builder () . usingServer ( \"http://localhost\" ) . withCapabilities ( capabilities ) . build (); try { await driver . get ( 'http://www.google.com' ); } finally { await driver . quit (); } })(); Kotlin firefoxOptions = FirefoxOptions () driver : WebDriver = new RemoteWebDriver ( new URL ( \"http://localhost\" ), firefoxOptions ) driver . get ( \"http://www.google.com\" ) driver . quit () Attention When you used Kubernetes you may have to forward the grid service to your local computer for the example code to work. For details on accessing your WebGrid within a cluster consult the Kubernetes specific docs.","title":"Getting started"},{"location":"getting-started/#getting-started","text":"Below are guides to get you started as quickly as possible on your specific platform! They provide a set of sane defaults which can later be tweaked for your use-case.","title":"Getting started"},{"location":"getting-started/#docker","text":"If you want to run a grid locally for simple testing purposes that require multiple isolated browsers or just want to evaluate this tool this is the right choice. Make sure you have Docker installed and configured properly. Before you can start the grid in Docker you have to create a network and volume for it: docker volume create webgrid docker network create webgrid Next you need to download the latest docker-compose.yml and run it: # Download compose file curl -fsSLO https://webgrid.dev/docker-compose.yml # Launch the grid docker-compose up Continue reading below on how to send requests to your grid. Todo The compose file refers locally built images. Replace them with hub.docker.com ones once the repository goes public!","title":"Docker"},{"location":"getting-started/#kubernetes","text":"WebGrid provides a Helm chart to get started as quickly as possible. Below is a guide on how to add the chart repository and install the chart. You can change the name of the release in the second command or add other options like the target namespace \u2014 for more details consult the Helm documentation. # Add the repository helm repo add webgrid https://webgrid.dev/ # Install the chart helm install example webgrid/webgrid","title":"Kubernetes"},{"location":"getting-started/#using-the-grid","text":"Once you have started the grid you can send requests to it using the regular Selenium client libraries available here . Java FirefoxOptions firefoxOptions = new FirefoxOptions (); WebDriver driver = new RemoteWebDriver ( new URL ( \"http://localhost\" ), firefoxOptions ); driver . get ( \"http://www.google.com\" ); driver . quit (); Python from selenium import webdriver firefox_options = webdriver . FirefoxOptions () driver = webdriver . Remote ( command_executor = 'http://localhost' , options = firefox_options ) driver . get ( \"http://www.google.com\" ) driver . quit () C# FirefoxOptions firefoxOptions = new FirefoxOptions (); IWebDriver driver = new RemoteWebDriver ( new Uri ( \"http://localhost\" ), firefoxOptions ); driver . Navigate (). GoToUrl ( \"http://www.google.com\" ); driver . Quit (); Ruby require 'selenium-webdriver' driver = Selenium :: WebDriver . for :remote , url : \"http://localhost\" , desired_capabilities : :firefox driver . get \"http://www.google.com\" driver . close JavaScript const { Builder , Capabilities } = require ( \"selenium-webdriver\" ); var capabilities = Capabilities . firefox (); ( async function helloSelenium () { let driver = new Builder () . usingServer ( \"http://localhost\" ) . withCapabilities ( capabilities ) . build (); try { await driver . get ( 'http://www.google.com' ); } finally { await driver . quit (); } })(); Kotlin firefoxOptions = FirefoxOptions () driver : WebDriver = new RemoteWebDriver ( new URL ( \"http://localhost\" ), firefoxOptions ) driver . get ( \"http://www.google.com\" ) driver . quit () Attention When you used Kubernetes you may have to forward the grid service to your local computer for the example code to work. For details on accessing your WebGrid within a cluster consult the Kubernetes specific docs.","title":"Using the grid"},{"location":"architecture/","text":"Overview \u00b6 No overview of the architecture exists yet :( Feel free to explore the following pages on your own! You can find the core documentation here","title":"Overview"},{"location":"architecture/#overview","text":"No overview of the architecture exists yet :( Feel free to explore the following pages on your own! You can find the core documentation here","title":"Overview"},{"location":"architecture/database/","text":"Database \u00b6 All metadata is stored in a key-value in-memory database called Redis . Below is a list of all keys that are currently in use, annotated with their type and format (if applicable). Root lists \u00b6 `orchestrators` = Set < string > // uuids `managers` = Set < string > // uuids `sessions.active` = Set < string > // uuids `sessions.terminated` = Set < string > // uuids Configuration \u00b6 `timeouts` = Hashes { queue = number // seconds scheduling = number // seconds nodeStartup = number // seconds driverStartup = number // seconds sessionTermination = number // seconds slotReclaimInterval = number // seconds } Storage \u00b6 // SID = storage ID // PID = randomly generated ephemeral provider ID `storage: ${ SID } : ${ PID } :host` = string EX 60 s // `${host}:${port}` API \u00b6 // AID = randomly generated ephemeral server ID `api: ${ AID } :host` = string EX 60 s ( host + port ) Sessions \u00b6 // ID = unique, external session identifier `session: ${ ID } :heartbeat.node` = string EX 60 s // RFC 3339 `session: ${ ID } :heartbeat.manager` = string EX 30 s // RFC 3339 `session: ${ ID } :slot` = string // slot ID `session: ${ ID } :orchestrator` = List < string > // orchestrator ID `session: ${ ID } :log` = Stream { component = 'node' | 'orchestrator' | 'manager' | 'proxy' level = 'info' | 'warn' | 'fail' code = string // event type, see below meta = string // additional information } `session: ${ ID } :status` = Hashes { queuedAt = string // RFC 3339 pendingAt = string // RFC 3339 aliveAt = string // RFC 3339 terminatedAt = string // RFC 3339 } `session: ${ ID } :capabilities` = Hashes { requested = string // JSON actual = string // JSON } `session: ${ ID } :upstream` = Hashes { host = string port = number driverSessionID = string } `session: ${ ID } :downstream` = Hashes { host = string userAgent = string lastSeen = string // RFC 3339 } `session: ${ ID } :storage` = string // storage ID Log event codes \u00b6 During the lifecycle of a session each component generates status codes for tracing purposes. Node \u00b6 Level Code Description Info BOOT node has become active DSTART driver in startup DALIVE driver has become responsive LSINIT local session created CLOSED session closed by downstream client HALT node enters shutdown Fail DTIMEOUT driver has not become responsive DFAILURE driver process reported an error STIMEOUT session has been inactive too long TERM node terminates due to fault condition Orchestrator \u00b6 Level Code Description Info SCHED node is being scheduled for startup Fail STARTFAIL creation/startup failure Manager \u00b6 Level Code Description Info QUEUED session has been queued at orchestrators NALLOC node slot has been allocated PENDING awaiting node startup NALIVE node has become responsive, client served Warn CLEFT client left before scheduling completed Fail INVALIDCAP invalid capabilities requested QUNAVAILABLE no orchestrator can satisfy the capabilities QTIMEOUT timed out waiting in queue OTIMEOUT timed out waiting for orchestrator to schedule node NTIMEOUT timed out waiting for node to become responsive Orchestrators \u00b6 `orchestrator: ${ ID } ` = Hashes { type = 'local' | 'docker' | 'k8s' } `orchestrator: ${ ID } :heartbeat` = number EX 60 `orchestrator: ${ ID } :capabilities:platformName` = string `orchestrator: ${ ID } :capabilities:browsers` = Set < string > // explained below `orchestrator: ${ ID } :slots.reclaimed` = List < string > // slot ID `orchestrator: ${ ID } :slots.available` = List < string > // slot ID `orchestrator: ${ ID } :slots` = Set < string > // slot ID `orchestrator: ${ ID } :backlog` = List < string > // session ID `orchestrator: ${ ID } :pending` = List < string > // session ID *Browsers are represented by a string containing the browserName and browserVersion separated by :: . For example chrome::81.0.4044.113 or firefox::74.0.1 . Reliable queue documentation Manager \u00b6 `manager: ${ ID } :heartbeat` = number EX 120 `manager: ${ ID } ` = Hashes { host = string port = number } Metrics \u00b6 HTTP (at proxy) \u00b6 `metrics:http:requestsTotal: ${ method } ` = Hashes { < http - status - code > = number } `metrics:http:net.bytes.total` = Hashes { in = number out = number } Sessions \u00b6 `metrics:sessions:total` = Hashes { queued = number pending = number alive = number terminated = number } // TODO Figure out how to actually set this, maybe on state change based on the previous state? `metrics:sessions:duration.seconds.total` = Hashes { queued = number pending = number alive = number } `metrics:sessions:startup.histogram:count` `metrics:sessions:startup.histogram:sum` `metrics:sessions:startup.histogram:buckets` = Hashes { < bucket > = number } `metrics:sessions:log: ${ level } ` = Hashes { < session - log - code > = number } Slots \u00b6 `metrics:slots:reclaimed.total` = Hashes { dead = number orphaned = number }","title":"Database"},{"location":"architecture/database/#database","text":"All metadata is stored in a key-value in-memory database called Redis . Below is a list of all keys that are currently in use, annotated with their type and format (if applicable).","title":"Database"},{"location":"architecture/database/#root-lists","text":"`orchestrators` = Set < string > // uuids `managers` = Set < string > // uuids `sessions.active` = Set < string > // uuids `sessions.terminated` = Set < string > // uuids","title":"Root lists"},{"location":"architecture/database/#configuration","text":"`timeouts` = Hashes { queue = number // seconds scheduling = number // seconds nodeStartup = number // seconds driverStartup = number // seconds sessionTermination = number // seconds slotReclaimInterval = number // seconds }","title":"Configuration"},{"location":"architecture/database/#storage","text":"// SID = storage ID // PID = randomly generated ephemeral provider ID `storage: ${ SID } : ${ PID } :host` = string EX 60 s // `${host}:${port}`","title":"Storage"},{"location":"architecture/database/#api","text":"// AID = randomly generated ephemeral server ID `api: ${ AID } :host` = string EX 60 s ( host + port )","title":"API"},{"location":"architecture/database/#sessions","text":"// ID = unique, external session identifier `session: ${ ID } :heartbeat.node` = string EX 60 s // RFC 3339 `session: ${ ID } :heartbeat.manager` = string EX 30 s // RFC 3339 `session: ${ ID } :slot` = string // slot ID `session: ${ ID } :orchestrator` = List < string > // orchestrator ID `session: ${ ID } :log` = Stream { component = 'node' | 'orchestrator' | 'manager' | 'proxy' level = 'info' | 'warn' | 'fail' code = string // event type, see below meta = string // additional information } `session: ${ ID } :status` = Hashes { queuedAt = string // RFC 3339 pendingAt = string // RFC 3339 aliveAt = string // RFC 3339 terminatedAt = string // RFC 3339 } `session: ${ ID } :capabilities` = Hashes { requested = string // JSON actual = string // JSON } `session: ${ ID } :upstream` = Hashes { host = string port = number driverSessionID = string } `session: ${ ID } :downstream` = Hashes { host = string userAgent = string lastSeen = string // RFC 3339 } `session: ${ ID } :storage` = string // storage ID","title":"Sessions"},{"location":"architecture/database/#log-event-codes","text":"During the lifecycle of a session each component generates status codes for tracing purposes.","title":"Log event codes"},{"location":"architecture/database/#node","text":"Level Code Description Info BOOT node has become active DSTART driver in startup DALIVE driver has become responsive LSINIT local session created CLOSED session closed by downstream client HALT node enters shutdown Fail DTIMEOUT driver has not become responsive DFAILURE driver process reported an error STIMEOUT session has been inactive too long TERM node terminates due to fault condition","title":"Node"},{"location":"architecture/database/#orchestrator","text":"Level Code Description Info SCHED node is being scheduled for startup Fail STARTFAIL creation/startup failure","title":"Orchestrator"},{"location":"architecture/database/#manager","text":"Level Code Description Info QUEUED session has been queued at orchestrators NALLOC node slot has been allocated PENDING awaiting node startup NALIVE node has become responsive, client served Warn CLEFT client left before scheduling completed Fail INVALIDCAP invalid capabilities requested QUNAVAILABLE no orchestrator can satisfy the capabilities QTIMEOUT timed out waiting in queue OTIMEOUT timed out waiting for orchestrator to schedule node NTIMEOUT timed out waiting for node to become responsive","title":"Manager"},{"location":"architecture/database/#orchestrators","text":"`orchestrator: ${ ID } ` = Hashes { type = 'local' | 'docker' | 'k8s' } `orchestrator: ${ ID } :heartbeat` = number EX 60 `orchestrator: ${ ID } :capabilities:platformName` = string `orchestrator: ${ ID } :capabilities:browsers` = Set < string > // explained below `orchestrator: ${ ID } :slots.reclaimed` = List < string > // slot ID `orchestrator: ${ ID } :slots.available` = List < string > // slot ID `orchestrator: ${ ID } :slots` = Set < string > // slot ID `orchestrator: ${ ID } :backlog` = List < string > // session ID `orchestrator: ${ ID } :pending` = List < string > // session ID *Browsers are represented by a string containing the browserName and browserVersion separated by :: . For example chrome::81.0.4044.113 or firefox::74.0.1 . Reliable queue documentation","title":"Orchestrators"},{"location":"architecture/database/#manager_1","text":"`manager: ${ ID } :heartbeat` = number EX 120 `manager: ${ ID } ` = Hashes { host = string port = number }","title":"Manager"},{"location":"architecture/database/#metrics","text":"","title":"Metrics"},{"location":"architecture/database/#http-at-proxy","text":"`metrics:http:requestsTotal: ${ method } ` = Hashes { < http - status - code > = number } `metrics:http:net.bytes.total` = Hashes { in = number out = number }","title":"HTTP (at proxy)"},{"location":"architecture/database/#sessions_1","text":"`metrics:sessions:total` = Hashes { queued = number pending = number alive = number terminated = number } // TODO Figure out how to actually set this, maybe on state change based on the previous state? `metrics:sessions:duration.seconds.total` = Hashes { queued = number pending = number alive = number } `metrics:sessions:startup.histogram:count` `metrics:sessions:startup.histogram:sum` `metrics:sessions:startup.histogram:buckets` = Hashes { < bucket > = number } `metrics:sessions:log: ${ level } ` = Hashes { < session - log - code > = number }","title":"Sessions"},{"location":"architecture/database/#slots","text":"`metrics:slots:reclaimed.total` = Hashes { dead = number orphaned = number }","title":"Slots"},{"location":"architecture/error-handling/","text":"Error handling \u00b6 In an ideal world, individual services are unable to fail by themselves. A service may reject a request as unfulfillable but always keeps processing. However due to the volatile nature of external resources like databases or cluster provisioners a monitoring system is required to handle outages. In this case a job system has been implemented where each job can have a dependency on an external resource. If this external resource dies, all dependent jobs will be cancelled. All root jobs of a component may then be restarted once the required resources become available again. Below is a rough outline of the architectural concepts involved. Parameter \u00b6 Parameters are constant values set at runtime, usually through the environment or command-line arguments. Resource \u00b6 Resources represent external data handlers like Redis, K8s or Storage. There are two types of resources, stateful and stateless. Stateful resources may report their current availability status while stateless ones are always reported as available. Resources have associated structures called Providers. A provider is responsible for creating handles to a resource. Each requested handle must have an associated ID. These IDs may not be unique if the underlying connection is shared. Handle IDs can be used for dependency tracking and job restarts in case a handle dies. Resources may have service-specific initialization jobs that are executed once they become available. These can be set on the corresponding resource provider upon creation. Contract \u00b6 Contracts atomically define behavior through a set of given input resource states and expected outputs. Contracts use placeholders for actual values much like class definitions. The list of contracts for this application can be found in the contracts directory . Request \u00b6 A specific instance of a contract with bound values is called a request. Job \u00b6 Jobs serve to fulfill one contract and may repeatedly do so. A job is considered healthy if it is able to fulfill its assigned contract. Task \u00b6 Jobs may have children called Tasks that process a single instance of a contract. For example the job could be an HTTP server which schedules new tasks for each incoming request. These children may then be terminated by the service if their resource handles go stale and the job may then take corresponding actions e.g. send an error code. Services \u00b6 Services are units that manage a set of related jobs and hold the required resource providers which can be configured through parameters. They are responsible for starting jobs and terminating those whose resources became stale. Jobs are restarted if the required resources are available again. Each job receives a handle to spawn ephemeral tasks which are not respawned. Additionally a future is passed that signals a clean shutdown condition \u2014 for proper SIGTERM handling.","title":"Error handling"},{"location":"architecture/error-handling/#error-handling","text":"In an ideal world, individual services are unable to fail by themselves. A service may reject a request as unfulfillable but always keeps processing. However due to the volatile nature of external resources like databases or cluster provisioners a monitoring system is required to handle outages. In this case a job system has been implemented where each job can have a dependency on an external resource. If this external resource dies, all dependent jobs will be cancelled. All root jobs of a component may then be restarted once the required resources become available again. Below is a rough outline of the architectural concepts involved.","title":"Error handling"},{"location":"architecture/error-handling/#parameter","text":"Parameters are constant values set at runtime, usually through the environment or command-line arguments.","title":"Parameter"},{"location":"architecture/error-handling/#resource","text":"Resources represent external data handlers like Redis, K8s or Storage. There are two types of resources, stateful and stateless. Stateful resources may report their current availability status while stateless ones are always reported as available. Resources have associated structures called Providers. A provider is responsible for creating handles to a resource. Each requested handle must have an associated ID. These IDs may not be unique if the underlying connection is shared. Handle IDs can be used for dependency tracking and job restarts in case a handle dies. Resources may have service-specific initialization jobs that are executed once they become available. These can be set on the corresponding resource provider upon creation.","title":"Resource"},{"location":"architecture/error-handling/#contract","text":"Contracts atomically define behavior through a set of given input resource states and expected outputs. Contracts use placeholders for actual values much like class definitions. The list of contracts for this application can be found in the contracts directory .","title":"Contract"},{"location":"architecture/error-handling/#request","text":"A specific instance of a contract with bound values is called a request.","title":"Request"},{"location":"architecture/error-handling/#job","text":"Jobs serve to fulfill one contract and may repeatedly do so. A job is considered healthy if it is able to fulfill its assigned contract.","title":"Job"},{"location":"architecture/error-handling/#task","text":"Jobs may have children called Tasks that process a single instance of a contract. For example the job could be an HTTP server which schedules new tasks for each incoming request. These children may then be terminated by the service if their resource handles go stale and the job may then take corresponding actions e.g. send an error code.","title":"Task"},{"location":"architecture/error-handling/#services","text":"Services are units that manage a set of related jobs and hold the required resource providers which can be configured through parameters. They are responsible for starting jobs and terminating those whose resources became stale. Jobs are restarted if the required resources are available again. Each job receives a handle to spawn ephemeral tasks which are not respawned. Additionally a future is passed that signals a clean shutdown condition \u2014 for proper SIGTERM handling.","title":"Services"},{"location":"architecture/services/","text":"Core services \u00b6 The WebGrid core consist of multiple services which interoperate to provide the reliable scaling functionality. Namely these are: Node Orchestrator (+ Provisioner) Manager Storage Proxy Below is a detailed description of the high-level responsibility of each service and their logical layout. The ones marked with a red asterisk can be scaled on-demand. Node \u00b6 At the core of each WebGrid are desktop environments which contain a Browser and additional software like the WebDriver[^e.g. geckodriver for Firefox] and a video encoder. The concept of a Node has been employed to abstract the underlying hardware & software logic of controlling the desktop environment, recording software and WebDriver away, leaving a clean interface for the Proxy[^Note that it rewrites incoming requests to change the external session ID to the one assigned by the local WebDriver]. Nodes start the driver, provide lifecycle information about the running session and handle cleanup when the session terminated. They have a very linear and simple service lifecycle: Start local driver Start screen recording Handle and forward WebDriver requests Terminate It does not matter whether or not a Node is running bare-bones on your Desktop or contained within larger infrastructure like a Kubernetes Cluster as long as the Proxy can reach it. Requests are forwarded to it through the proxy at * /session/${SID}/* Orchestrator \u00b6 Since the lifespan of a Node is tied to one specific session there needs to be an instance which handles the creation and deletion of them. The Orchestrator receives instructions from the Manager to create new Nodes and returns information on how to reach them. An Orchestrator can have multiple Node configurations available to choose from (e.g. different Browsers) and provides these capabilities to the Manager as a HashMap. Usually Orchestrators parse and forward the requests to create new nodes to a Provisioner like Docker or K8s however they can also just launch a local process. Slots \u00b6 One given Orchestrator may not have an unlimited amount of resources at its disposal. To constrain the number of Nodes that can exist at one given time, controlled by one given Orchestrator a concept of slots has been introduced. They are basically just IDs which are generated when the service is created and resemble \"Coupons\" which the Manager can use to request the creation of a Node. A slot is associated with one session on creation, if available and stays bound until the session is terminated at which point it is returned into the list of available slots for a new session to retrieve. Since any given service may die while processing requests each slot has a \"parent\" which is responsible for it. While a Node is running, it has the responsibility for its own slot. If it dies unexpectedly (and thus its heartbeat ceases to exist) the Orchestrator may reclaim its slot, adding it back to the list of available ones. This process is explained in more detail in the Workflows document. Here is an image of the slot lifecycle: Manager \u00b6 The Manager processes requests from clients to create new sessions by determining which orchestrators match the requested capabilities, requesting a slot and verifying the Node scheduling and startup process. A session leaves the responsibility of the manager as soon as the startup sequence has completed. For more details consult the Scheduling Workflow . It receives requests through the proxy at POST /session Storage \u00b6 In order to access files that sessions stored on disk a server instance with access to them is needed. For this purpose a storage service exists which delivers resources like screen recordings and manages storage occupancy by monitoring a local directory and potentially running cleanup tasks based on a local SQLite database. It receives requests through the proxy at GET /storage/${SID}/* . Proxy \u00b6 In order to route session traffic to the node that is hosting the session a reverse proxy is required. For this purpose (and load-balancing in a scenario with more than one manager instance) a service has been added. It listens on Redis key-space notifications to detect managers and nodes coming online or going offline, determined by their heartbeat. When a node comes online, the proxy routes all requests of the corresponding session ( /session/<ID>* ) to the node. For managers it routes requests matching POST /session to a randomly selected upstream from the list of alive managers.","title":"Core services"},{"location":"architecture/services/#core-services","text":"The WebGrid core consist of multiple services which interoperate to provide the reliable scaling functionality. Namely these are: Node Orchestrator (+ Provisioner) Manager Storage Proxy Below is a detailed description of the high-level responsibility of each service and their logical layout. The ones marked with a red asterisk can be scaled on-demand.","title":"Core services"},{"location":"architecture/services/#node","text":"At the core of each WebGrid are desktop environments which contain a Browser and additional software like the WebDriver[^e.g. geckodriver for Firefox] and a video encoder. The concept of a Node has been employed to abstract the underlying hardware & software logic of controlling the desktop environment, recording software and WebDriver away, leaving a clean interface for the Proxy[^Note that it rewrites incoming requests to change the external session ID to the one assigned by the local WebDriver]. Nodes start the driver, provide lifecycle information about the running session and handle cleanup when the session terminated. They have a very linear and simple service lifecycle: Start local driver Start screen recording Handle and forward WebDriver requests Terminate It does not matter whether or not a Node is running bare-bones on your Desktop or contained within larger infrastructure like a Kubernetes Cluster as long as the Proxy can reach it. Requests are forwarded to it through the proxy at * /session/${SID}/*","title":"Node"},{"location":"architecture/services/#orchestrator","text":"Since the lifespan of a Node is tied to one specific session there needs to be an instance which handles the creation and deletion of them. The Orchestrator receives instructions from the Manager to create new Nodes and returns information on how to reach them. An Orchestrator can have multiple Node configurations available to choose from (e.g. different Browsers) and provides these capabilities to the Manager as a HashMap. Usually Orchestrators parse and forward the requests to create new nodes to a Provisioner like Docker or K8s however they can also just launch a local process.","title":"Orchestrator"},{"location":"architecture/services/#slots","text":"One given Orchestrator may not have an unlimited amount of resources at its disposal. To constrain the number of Nodes that can exist at one given time, controlled by one given Orchestrator a concept of slots has been introduced. They are basically just IDs which are generated when the service is created and resemble \"Coupons\" which the Manager can use to request the creation of a Node. A slot is associated with one session on creation, if available and stays bound until the session is terminated at which point it is returned into the list of available slots for a new session to retrieve. Since any given service may die while processing requests each slot has a \"parent\" which is responsible for it. While a Node is running, it has the responsibility for its own slot. If it dies unexpectedly (and thus its heartbeat ceases to exist) the Orchestrator may reclaim its slot, adding it back to the list of available ones. This process is explained in more detail in the Workflows document. Here is an image of the slot lifecycle:","title":"Slots"},{"location":"architecture/services/#manager","text":"The Manager processes requests from clients to create new sessions by determining which orchestrators match the requested capabilities, requesting a slot and verifying the Node scheduling and startup process. A session leaves the responsibility of the manager as soon as the startup sequence has completed. For more details consult the Scheduling Workflow . It receives requests through the proxy at POST /session","title":"Manager"},{"location":"architecture/services/#storage","text":"In order to access files that sessions stored on disk a server instance with access to them is needed. For this purpose a storage service exists which delivers resources like screen recordings and manages storage occupancy by monitoring a local directory and potentially running cleanup tasks based on a local SQLite database. It receives requests through the proxy at GET /storage/${SID}/* .","title":"Storage"},{"location":"architecture/services/#proxy","text":"In order to route session traffic to the node that is hosting the session a reverse proxy is required. For this purpose (and load-balancing in a scenario with more than one manager instance) a service has been added. It listens on Redis key-space notifications to detect managers and nodes coming online or going offline, determined by their heartbeat. When a node comes online, the proxy routes all requests of the corresponding session ( /session/<ID>* ) to the node. For managers it routes requests matching POST /session to a randomly selected upstream from the list of alive managers.","title":"Proxy"},{"location":"architecture/structure/","text":"Project structure \u00b6 This page covers various topics that are required to understand the project structure both on a file-system as well as a conceptual level. Repository layout \u00b6 The repository contains multiple subfolders: /docs This documentation /core Core component /api API component /distribution Platform specific packaging scripts docker Docker images and stuff kubernetes Helm chart High level concepts \u00b6 The project has been divided into multiple hierarchical levels of abstraction. They are outlined in top-to-bottom order below. Components \u00b6 At the very top there are components which rest at the root directory of the project. These each provide a comprehensive feature-set of the grid and would be totally isolated in an ideal world. However, as some data sharing is required they access each other's data sources through clearly defined interfaces. Below are all components that are currently in existence and planned: Core \u00b6 The Core component houses all services that are critical to the operation of the grid and need to be redundant, resilient and highly performant. It is written in Rust to achieve these goals. API \u00b6 The API component provides an external interface to the internal metadata of the grid. As of now it is read-only and available through the Core components Proxy service. It is written in TypeScript and uses GraphQL as the query language. Dashboard \u00b6 A future component that is on the roadmap will be the Dashboard . It is expected to provide a comprehensive and user-friendly overview of the grid's status. Current plans are for it to be written using Svelte . Services \u00b6 Services are lower level puzzle pieces that each serve a distinct role and can be scaled individually. Currently, only the Core component makes use of this concept as every other component would only have one service. The Core services are described seperately . Jobs & Tasks \u00b6 In order to improve error handling and recovery a concept of Jobs & Tasks has been introduced. Each service process internally launches a set of jobs which each serve a single purpose e.g. serving HTTP POST requests from selenium clients in case of the manager. Each Job may spawn tasks which are ephemeral units that run once to e.g. serve an incoming request. To read more about jobs and how they improve error processing, head to the error handling page .","title":"Project structure"},{"location":"architecture/structure/#project-structure","text":"This page covers various topics that are required to understand the project structure both on a file-system as well as a conceptual level.","title":"Project structure"},{"location":"architecture/structure/#repository-layout","text":"The repository contains multiple subfolders: /docs This documentation /core Core component /api API component /distribution Platform specific packaging scripts docker Docker images and stuff kubernetes Helm chart","title":"Repository layout"},{"location":"architecture/structure/#high-level-concepts","text":"The project has been divided into multiple hierarchical levels of abstraction. They are outlined in top-to-bottom order below.","title":"High level concepts"},{"location":"architecture/structure/#components","text":"At the very top there are components which rest at the root directory of the project. These each provide a comprehensive feature-set of the grid and would be totally isolated in an ideal world. However, as some data sharing is required they access each other's data sources through clearly defined interfaces. Below are all components that are currently in existence and planned:","title":"Components"},{"location":"architecture/structure/#core","text":"The Core component houses all services that are critical to the operation of the grid and need to be redundant, resilient and highly performant. It is written in Rust to achieve these goals.","title":"Core"},{"location":"architecture/structure/#api","text":"The API component provides an external interface to the internal metadata of the grid. As of now it is read-only and available through the Core components Proxy service. It is written in TypeScript and uses GraphQL as the query language.","title":"API"},{"location":"architecture/structure/#dashboard","text":"A future component that is on the roadmap will be the Dashboard . It is expected to provide a comprehensive and user-friendly overview of the grid's status. Current plans are for it to be written using Svelte .","title":"Dashboard"},{"location":"architecture/structure/#services","text":"Services are lower level puzzle pieces that each serve a distinct role and can be scaled individually. Currently, only the Core component makes use of this concept as every other component would only have one service. The Core services are described seperately .","title":"Services"},{"location":"architecture/structure/#jobs-tasks","text":"In order to improve error handling and recovery a concept of Jobs & Tasks has been introduced. Each service process internally launches a set of jobs which each serve a single purpose e.g. serving HTTP POST requests from selenium clients in case of the manager. Each Job may spawn tasks which are ephemeral units that run once to e.g. serve an incoming request. To read more about jobs and how they improve error processing, head to the error handling page .","title":"Jobs &amp; Tasks"},{"location":"architecture/workflows/","text":"Workflows \u00b6 Below is a list of common process workflows that happen within WebGrid. They may help understanding how the grid operates internally. Scheduling workflow \u00b6 Orchestrator puts available slots into :slots.available on startup by running a reclaim cycle Manager receives client request, runs session creation workflow Manager pulls slot from orchestrators into session:<ID>:slot This operation has to be implemented using BLPOP and thus might crash between the BLPOP and SET operations, making the slot vanish. If this happens the orchestrators reclaim cycle may make this slot available again (more on that later) Manager pushes the sessionID into the orchestrators :backlog Orchestrator sequentially processes the backlog, moving messages into pending while provisioning the nodes Orchestrator notifies the manager Removes the sessionID from pending Sets the sessions :status:pendingAt Pushes its ID into the sessions :orchestrator key Manager watches the sessions :orchestrator key using BRPOPLPUSH for it to become available Manager runs health-check against node ( http://node/status ) Manager replies to client Manager sets :status:aliveAt property, effectively moving slot responsibility to the node from now on Node runs session termination workflow when client ends session If the orchestrator finds any sessions in its pending list on startup, it may fulfil these requests if the referenced session has not entered a terminated state (the manager ran into a timeout waiting for the orchestrator). Otherwise it may delete the task. Slot reclaim workflow \u00b6 Since any given component in the system may fail during slot interaction the orchestrator has to reclaim slots that have become stale/unused but are not properly returned. In order to do so it follows these steps, keeping an internal list of reclaimed slot-IDs: Iterate sessions that have not entered a terminated state and whose slot is owned by this orchestrator If :status:aliveAt is not set and there is no :heartbeat.manager , run session termination workflow Else if there is no :heartbeat.node , run session termination workflow Else the slot is still in use If not all slots are either reclaimed or in use, the missing slots may be added back to the list of available slots This reclaim cycle may be used as a cleanup cycle by the orchestrator to terminate any orphaned nodes that are not referenced by an alive session in the database. Changing the number of slots for an orchestrator \u00b6 This is done by the orchestrator on startup (target value is read from its configuration/environment). Adding new slots \u00b6 The number of slots may be increased by simply adding slots to orchestrator:<ID>:slots and orchestrator:<ID>:slots.available Removing active slots \u00b6 To decrease the slot amount, queue a number of BRPOP statements on the :slots.available and remove the returned slots from the :slots list Session creation workflow \u00b6 Generate ID Write initial information session:<ID>:status:queuedAt session:<ID>:capabilities:requested session:<ID>:downstream:* Append sessionID to sessions.active Session termination workflow \u00b6 Move sessions :slot back into orchestrators :slots.available Move sessions ID from sessions.active to sessions.terminated Set sessions :status:terminatedAt to the current time Delete :heartbeat.node","title":"Workflows"},{"location":"architecture/workflows/#workflows","text":"Below is a list of common process workflows that happen within WebGrid. They may help understanding how the grid operates internally.","title":"Workflows"},{"location":"architecture/workflows/#scheduling-workflow","text":"Orchestrator puts available slots into :slots.available on startup by running a reclaim cycle Manager receives client request, runs session creation workflow Manager pulls slot from orchestrators into session:<ID>:slot This operation has to be implemented using BLPOP and thus might crash between the BLPOP and SET operations, making the slot vanish. If this happens the orchestrators reclaim cycle may make this slot available again (more on that later) Manager pushes the sessionID into the orchestrators :backlog Orchestrator sequentially processes the backlog, moving messages into pending while provisioning the nodes Orchestrator notifies the manager Removes the sessionID from pending Sets the sessions :status:pendingAt Pushes its ID into the sessions :orchestrator key Manager watches the sessions :orchestrator key using BRPOPLPUSH for it to become available Manager runs health-check against node ( http://node/status ) Manager replies to client Manager sets :status:aliveAt property, effectively moving slot responsibility to the node from now on Node runs session termination workflow when client ends session If the orchestrator finds any sessions in its pending list on startup, it may fulfil these requests if the referenced session has not entered a terminated state (the manager ran into a timeout waiting for the orchestrator). Otherwise it may delete the task.","title":"Scheduling workflow"},{"location":"architecture/workflows/#slot-reclaim-workflow","text":"Since any given component in the system may fail during slot interaction the orchestrator has to reclaim slots that have become stale/unused but are not properly returned. In order to do so it follows these steps, keeping an internal list of reclaimed slot-IDs: Iterate sessions that have not entered a terminated state and whose slot is owned by this orchestrator If :status:aliveAt is not set and there is no :heartbeat.manager , run session termination workflow Else if there is no :heartbeat.node , run session termination workflow Else the slot is still in use If not all slots are either reclaimed or in use, the missing slots may be added back to the list of available slots This reclaim cycle may be used as a cleanup cycle by the orchestrator to terminate any orphaned nodes that are not referenced by an alive session in the database.","title":"Slot reclaim workflow"},{"location":"architecture/workflows/#changing-the-number-of-slots-for-an-orchestrator","text":"This is done by the orchestrator on startup (target value is read from its configuration/environment).","title":"Changing the number of slots for an orchestrator"},{"location":"architecture/workflows/#adding-new-slots","text":"The number of slots may be increased by simply adding slots to orchestrator:<ID>:slots and orchestrator:<ID>:slots.available","title":"Adding new slots"},{"location":"architecture/workflows/#removing-active-slots","text":"To decrease the slot amount, queue a number of BRPOP statements on the :slots.available and remove the returned slots from the :slots list","title":"Removing active slots"},{"location":"architecture/workflows/#session-creation-workflow","text":"Generate ID Write initial information session:<ID>:status:queuedAt session:<ID>:capabilities:requested session:<ID>:downstream:* Append sessionID to sessions.active","title":"Session creation workflow"},{"location":"architecture/workflows/#session-termination-workflow","text":"Move sessions :slot back into orchestrators :slots.available Move sessions ID from sessions.active to sessions.terminated Set sessions :status:terminatedAt to the current time Delete :heartbeat.node","title":"Session termination workflow"},{"location":"contribute/","text":"Introduction \u00b6 First off, thank you for considering contributing to WebGrid! You are awesome and we greatly appreciate any contributions \ud83d\ude42 Awesome Minions GIF from Awesome GIFs Following the guidelines outlined on the next few pages helps us focusing on maintaining and developing this project and allowing more time to assist you with your contributions. So please spent the time to read the sections that correspond to your area of contribution carefully! WebGrid is an open source project and we love to receive contributions from our community \u2014 you! There are many ways to contribute, from writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests or writing code which can be incorporated into WebGrid itself. If you are unsure how to contribute to a open source project in general, take a look at this resource or this one . If you find yourself wishing for a feature that doesn't exist in WebGrid, you are probably not alone. There are bound to be others out there with similar needs. Many of the features that WebGrid has today have been added because our users saw the need. Open an issue on our issues list on GitHub which describes the feature you would like to see, why you need it, and how it should work. Lastly, please be nice to everyone! You can read the code of conduct here , please adhere to it we really don't want to get out the ban-hammer.","title":"Introduction"},{"location":"contribute/#introduction","text":"First off, thank you for considering contributing to WebGrid! You are awesome and we greatly appreciate any contributions \ud83d\ude42 Awesome Minions GIF from Awesome GIFs Following the guidelines outlined on the next few pages helps us focusing on maintaining and developing this project and allowing more time to assist you with your contributions. So please spent the time to read the sections that correspond to your area of contribution carefully! WebGrid is an open source project and we love to receive contributions from our community \u2014 you! There are many ways to contribute, from writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests or writing code which can be incorporated into WebGrid itself. If you are unsure how to contribute to a open source project in general, take a look at this resource or this one . If you find yourself wishing for a feature that doesn't exist in WebGrid, you are probably not alone. There are bound to be others out there with similar needs. Many of the features that WebGrid has today have been added because our users saw the need. Open an issue on our issues list on GitHub which describes the feature you would like to see, why you need it, and how it should work. Lastly, please be nice to everyone! You can read the code of conduct here , please adhere to it we really don't want to get out the ban-hammer.","title":"Introduction"},{"location":"contribute/code-contrib/","text":"Code contributions \u00b6 We greatly appreciate any code contributions even if it is just a small typo fix in the documentation. You can take a look at the issue tracker for Available tasks which already have a solution outlined. If you are unsure on how to implement a change just ask, we are there to guide you through the process. You can also take a look at the list of Accepted tasks . These are accepted changes that do not have a sketch on how to solve them yet but if you are interested in approaching one we are going to assist you in solving it! Just comment on the issue of interest to let us know. Tip Did you know you can edit any documentation page directly just by clicking the edit button on the top right of each page? Please feel free to do so if you have found areas of improvement! Getting to know the project \u00b6 It can be a daunting task to get into a new project, we encountered it ourselves more than we'd like to admit. For this reason a comprehensive guide to the project structure, local developer setup and other topics is available in the Architecture tab . We strive to make the onboarding experience as simple and straightforward as possible so if you have any questions or ideas for improving it please open a ticket ! Conventions \u00b6 This project follows a few conventions regarding code contributions \u2014 below is a list of them. Commit messages \u00b6 Your commit messages should always be imparative, capitalized, without any punctuation at the end and able to complete the following sentence: If applied, this commit will <your-commit-message>. You can read more about how to write good commit messages and why its important over here . This blog post is used as a guideline for this repository! Gitmoji \u00b6 All commit messages should be prefixed with a GitHub Emoji that describes in one character what the commit is all about. For reference you should take a look at the gitmoji page ! PGP signing \u00b6 Every commit message should be PGP signed. This can be achieved by either editing files directly on GitHub or setting up commit signing locally (make sure to upload your public key to GitHub if you sign locally). Workflow \u00b6 Below are steps which are usually followed for code contributions \u2014 use them to get acquainted to the process before contributing or if you are unsure on what follows next. 1. Solution sketch \u00b6 At the beginning of each code contribution a solution sketch has to exist. For some tasks like typo fixes this is a no-brainer but more complex tasks require some discussion up-front on how to approach a problem. This ensures that the result is compliant with project standards and doesn't create unexpected problems down the road! For issues from the Available category a solution sketch is already provided which makes it even easier to get started! 2. Implementing changes \u00b6 Once a solution sketch has been outlined you can start working on the code. Make sure to notify the maintainers that you have started work so that the issue can be moved to the corresponding lifecycle stage so no work is done twice. At this stage you setup your development environment (which may or may not be required depending on the changes as e.g. documentation changes can usually be done online using the GitHub Web Editor) and write code. If you have any questions ask on your issue ticket or submit a Draft Pull Request and comment on it if you need code-level assistance! 3. Code review \u00b6 If you are done with your changes or want to seek feedback from maintainers you push your changes and open a Pull Request. Make sure to add the original issue number to the description so it can be associated later. Take special considerations in naming your PR as this message will be published in the Changelog \ud83d\ude09 The PR will be reviewed by other project members and automatic tests are run against your code. If everything is well, the changes will be approved and merged into the main branch. However, it is very common that changes are requested (don't feel bad about it, we provide you feedback to improve your already amazing contribution not to make it look bad) which moves you back to the previous stage of Implementing changes .","title":"Code contributions"},{"location":"contribute/code-contrib/#code-contributions","text":"We greatly appreciate any code contributions even if it is just a small typo fix in the documentation. You can take a look at the issue tracker for Available tasks which already have a solution outlined. If you are unsure on how to implement a change just ask, we are there to guide you through the process. You can also take a look at the list of Accepted tasks . These are accepted changes that do not have a sketch on how to solve them yet but if you are interested in approaching one we are going to assist you in solving it! Just comment on the issue of interest to let us know. Tip Did you know you can edit any documentation page directly just by clicking the edit button on the top right of each page? Please feel free to do so if you have found areas of improvement!","title":"Code contributions"},{"location":"contribute/code-contrib/#getting-to-know-the-project","text":"It can be a daunting task to get into a new project, we encountered it ourselves more than we'd like to admit. For this reason a comprehensive guide to the project structure, local developer setup and other topics is available in the Architecture tab . We strive to make the onboarding experience as simple and straightforward as possible so if you have any questions or ideas for improving it please open a ticket !","title":"Getting to know the project"},{"location":"contribute/code-contrib/#conventions","text":"This project follows a few conventions regarding code contributions \u2014 below is a list of them.","title":"Conventions"},{"location":"contribute/code-contrib/#commit-messages","text":"Your commit messages should always be imparative, capitalized, without any punctuation at the end and able to complete the following sentence: If applied, this commit will <your-commit-message>. You can read more about how to write good commit messages and why its important over here . This blog post is used as a guideline for this repository!","title":"Commit messages"},{"location":"contribute/code-contrib/#gitmoji","text":"All commit messages should be prefixed with a GitHub Emoji that describes in one character what the commit is all about. For reference you should take a look at the gitmoji page !","title":"Gitmoji"},{"location":"contribute/code-contrib/#pgp-signing","text":"Every commit message should be PGP signed. This can be achieved by either editing files directly on GitHub or setting up commit signing locally (make sure to upload your public key to GitHub if you sign locally).","title":"PGP signing"},{"location":"contribute/code-contrib/#workflow","text":"Below are steps which are usually followed for code contributions \u2014 use them to get acquainted to the process before contributing or if you are unsure on what follows next.","title":"Workflow"},{"location":"contribute/code-contrib/#1-solution-sketch","text":"At the beginning of each code contribution a solution sketch has to exist. For some tasks like typo fixes this is a no-brainer but more complex tasks require some discussion up-front on how to approach a problem. This ensures that the result is compliant with project standards and doesn't create unexpected problems down the road! For issues from the Available category a solution sketch is already provided which makes it even easier to get started!","title":"1. Solution sketch"},{"location":"contribute/code-contrib/#2-implementing-changes","text":"Once a solution sketch has been outlined you can start working on the code. Make sure to notify the maintainers that you have started work so that the issue can be moved to the corresponding lifecycle stage so no work is done twice. At this stage you setup your development environment (which may or may not be required depending on the changes as e.g. documentation changes can usually be done online using the GitHub Web Editor) and write code. If you have any questions ask on your issue ticket or submit a Draft Pull Request and comment on it if you need code-level assistance!","title":"2. Implementing changes"},{"location":"contribute/code-contrib/#3-code-review","text":"If you are done with your changes or want to seek feedback from maintainers you push your changes and open a Pull Request. Make sure to add the original issue number to the description so it can be associated later. Take special considerations in naming your PR as this message will be published in the Changelog \ud83d\ude09 The PR will be reviewed by other project members and automatic tests are run against your code. If everything is well, the changes will be approved and merged into the main branch. However, it is very common that changes are requested (don't feel bad about it, we provide you feedback to improve your already amazing contribution not to make it look bad) which moves you back to the previous stage of Implementing changes .","title":"3. Code review"},{"location":"contribute/code-of-conduct/","text":"Contributor Covenant Code of Conduct \u00b6 Our Pledge \u00b6 We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards \u00b6 Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities \u00b6 Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope \u00b6 This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement \u00b6 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at til@blechschmidt.dev . All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines \u00b6 Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction \u00b6 Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning \u00b6 Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban \u00b6 Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban \u00b6 Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution \u00b6 This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Code of conduct"},{"location":"contribute/code-of-conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"contribute/code-of-conduct/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"contribute/code-of-conduct/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"contribute/code-of-conduct/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"contribute/code-of-conduct/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"contribute/code-of-conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at til@blechschmidt.dev . All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"contribute/code-of-conduct/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"contribute/code-of-conduct/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"contribute/code-of-conduct/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"contribute/code-of-conduct/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"contribute/code-of-conduct/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"contribute/code-of-conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Attribution"},{"location":"contribute/issues/","text":"Issue reporting \u00b6 Creating an issue is the easiest way to contribute to the project! All issues are tracked using the GitHub Issue Board . There are four different types of issues: Type Description Issues that affect the functionality of the software Feature requests or ideas that would improve the project Improvements to code quality, readability, documentation or architecture General questions about the project, how it works and for non-documented features Reporting \u00b6 If you have a question, encountered a bug or want to suggest a feature please don't hesitate to open a ticket \u2014 Community feedback helps the project grow and adapt in the best possible way! Preventing duplicates Make sure to browse the issue backlog and previously closed issues as well as the FAQ to see if somebody else already covered your topic! This helps us focus on implementing your ideas and helping you with issues instead of housekeeping duplicates. Lifecycle \u00b6 Each ticket goes through the following lifecycle outlined below: stateDiagram [*] --> Pending Pending --> Available Pending --> Accepted Accepted --> Available Available --> InProgress InProgress --> Completed InProgress --> Abandoned Pending --> Abandoned Accepted --> Abandoned Pending --> Duplicate Completed --> [*] Abandoned --> [*] Duplicate --> [*] In addition to the states shown above, it may enter one of the following states from almost any other state: Status Description Waiting for another issue to be resolved Waiting for external response/feedback to continue down the issue lifecycle The proposed solution needs to be revised and changed according to maintainer feedback A solution has been proposed and needs to be reviewed by a maintainer For a more detailed description of all possible states, consult the repositories label page . Pull requests \u00b6 In addition to the labels of issues, Pull Request may have additional labels which identify the type of version bump. Change Description New feature that breaks backwards compatibility and requires a manual upgrade Backwards compatible feature addition that can be upgraded automatically Minor changes like backwards compatible bug fixes and documentation changes","title":"Issue reporting"},{"location":"contribute/issues/#issue-reporting","text":"Creating an issue is the easiest way to contribute to the project! All issues are tracked using the GitHub Issue Board . There are four different types of issues: Type Description Issues that affect the functionality of the software Feature requests or ideas that would improve the project Improvements to code quality, readability, documentation or architecture General questions about the project, how it works and for non-documented features","title":"Issue reporting"},{"location":"contribute/issues/#reporting","text":"If you have a question, encountered a bug or want to suggest a feature please don't hesitate to open a ticket \u2014 Community feedback helps the project grow and adapt in the best possible way! Preventing duplicates Make sure to browse the issue backlog and previously closed issues as well as the FAQ to see if somebody else already covered your topic! This helps us focus on implementing your ideas and helping you with issues instead of housekeeping duplicates.","title":"Reporting"},{"location":"contribute/issues/#lifecycle","text":"Each ticket goes through the following lifecycle outlined below: stateDiagram [*] --> Pending Pending --> Available Pending --> Accepted Accepted --> Available Available --> InProgress InProgress --> Completed InProgress --> Abandoned Pending --> Abandoned Accepted --> Abandoned Pending --> Duplicate Completed --> [*] Abandoned --> [*] Duplicate --> [*] In addition to the states shown above, it may enter one of the following states from almost any other state: Status Description Waiting for another issue to be resolved Waiting for external response/feedback to continue down the issue lifecycle The proposed solution needs to be revised and changed according to maintainer feedback A solution has been proposed and needs to be reviewed by a maintainer For a more detailed description of all possible states, consult the repositories label page .","title":"Lifecycle"},{"location":"contribute/issues/#pull-requests","text":"In addition to the labels of issues, Pull Request may have additional labels which identify the type of version bump. Change Description New feature that breaks backwards compatibility and requires a manual upgrade Backwards compatible feature addition that can be upgraded automatically Minor changes like backwards compatible bug fixes and documentation changes","title":"Pull requests"},{"location":"contribute/release/","text":"Release workflow \u00b6 The project generally follows the SemVer 2.0 convention. A new release usually follows these steps: Contributors open PRs Changes get merged into main branch GitHub Actions creates a draft release and updates it after every contribution Once a large enough number of contributions have accumulated the draft is published GitHub Actions release pipeline takes the following steps Build all components Create and push Docker images Publish new documentation and Helm chart Attach executables to GitHub Release","title":"Release workflow"},{"location":"contribute/release/#release-workflow","text":"The project generally follows the SemVer 2.0 convention. A new release usually follows these steps: Contributors open PRs Changes get merged into main branch GitHub Actions creates a draft release and updates it after every contribution Once a large enough number of contributions have accumulated the draft is published GitHub Actions release pipeline takes the following steps Build all components Create and push Docker images Publish new documentation and Helm chart Attach executables to GitHub Release","title":"Release workflow"},{"location":"features/api/","text":"API \u00b6 The grid exposes an API which provides read-only access to the status and metadata of sessions and some other internal components which might be of interest. It uses the GraphQL query language to provide predictable and typed responses and is available under the following URL: http://<your-webgrid-address>/api Tip When opening the API in a browser, a GraphQL Playground opens up which provides syntax highlighting, code completion and a complete documentation!","title":"API"},{"location":"features/api/#api","text":"The grid exposes an API which provides read-only access to the status and metadata of sessions and some other internal components which might be of interest. It uses the GraphQL query language to provide predictable and typed responses and is available under the following URL: http://<your-webgrid-address>/api Tip When opening the API in a browser, a GraphQL Playground opens up which provides syntax highlighting, code completion and a complete documentation!","title":"API"},{"location":"features/hybrid-grid/","text":"Hybrid grid \u00b6 The Getting Started Guide covers basic setups in either Kubernetes or Docker. However, in certain scenarios it might be required to include other devices that can't be enslaved to a cluster. Imagine a software testing use-case where you have the following setup: Chrome & Firefox pods running in K8s Safari on external Mac Mini Edge on external Windows PC All these devices can be collated behind a single grid endpoint. In theory, every component of the grid can be hosted in and scaled to any device regardless of whether or not it is in the cluster or not. However, for larger instances it is recommended to run the central grid components in Kubernetes and add external devices to extend its capabilities with e.g. Safari Browsers. Requirements \u00b6 In order to add external devices a few requirements have to be met: Proxys and Manager pods have to be able to reach the device Redis database has to be accessible by the device Once these prerequisites are met, continue to the next sections. Local orchestrator \u00b6 The orchestrator service is responsible for scheduling resources like Kubernetes pods or Docker containers which then in turn run the browsers. To use a local browser like Safari a single-instance orchestrator is required. Todo This feature is on the horizon. Even though the implementation has not yet been started, it has been incorporated during the design stage and its complexity is rather mild. If you need this feature, please open up an Issue or +1 an existing one regarding this feature.","title":"Hybrid grid"},{"location":"features/hybrid-grid/#hybrid-grid","text":"The Getting Started Guide covers basic setups in either Kubernetes or Docker. However, in certain scenarios it might be required to include other devices that can't be enslaved to a cluster. Imagine a software testing use-case where you have the following setup: Chrome & Firefox pods running in K8s Safari on external Mac Mini Edge on external Windows PC All these devices can be collated behind a single grid endpoint. In theory, every component of the grid can be hosted in and scaled to any device regardless of whether or not it is in the cluster or not. However, for larger instances it is recommended to run the central grid components in Kubernetes and add external devices to extend its capabilities with e.g. Safari Browsers.","title":"Hybrid grid"},{"location":"features/hybrid-grid/#requirements","text":"In order to add external devices a few requirements have to be met: Proxys and Manager pods have to be able to reach the device Redis database has to be accessible by the device Once these prerequisites are met, continue to the next sections.","title":"Requirements"},{"location":"features/hybrid-grid/#local-orchestrator","text":"The orchestrator service is responsible for scheduling resources like Kubernetes pods or Docker containers which then in turn run the browsers. To use a local browser like Safari a single-instance orchestrator is required. Todo This feature is on the horizon. Even though the implementation has not yet been started, it has been incorporated during the design stage and its complexity is rather mild. If you need this feature, please open up an Issue or +1 an existing one regarding this feature.","title":"Local orchestrator"},{"location":"features/screen-recording/","text":"Screen recording \u00b6 The grid is capable of capturing the screen for each browser session you create. This feature is enabled by default in a local Docker instance, but requires some additional configuration in a cluster. Warning By default, this feature is disabled in the Kubernetes helm chart as it requires additional setup. Details on how to get up and running can be found on the Kubernetes storage page ! Session ID \u00b6 In order to view or embed a video you need to retrieve its unique session identifier. The simplest method is through your client library \u2014 below are a few examples: Java FirefoxOptions firefoxOptions = new FirefoxOptions (); WebDriver driver = new RemoteWebDriver ( new URL ( \"http://localhost\" ), firefoxOptions ); driver . get ( \"http://dcuk.com\" ); SessionId session = (( FirefoxDriver ) driver ). getSessionId (); System . out . println ( \"Session id: \" + session . toString ()); System . in . read (); driver . quit (); Python from selenium import webdriver firefox_options = webdriver . FirefoxOptions () driver = webdriver . Remote ( command_executor = 'http://localhost' , options = firefox_options ) driver . get ( \"http://www.google.com\" ) print ( \"Session id: \" + driver . session_id ); input ( \"Press Enter to quit...\" ) driver . quit () Retrieving session ID from API Alternatively, you can get a list of all (or just active) sessions from the grid API. Note however that the list can be very large, so it might be hard to identify yours. Use the following query to get a list of sessions e.g. by pasting it into the API explorer or using a GraphQL client: query { sessions { id alive } } It returns you a list of all sessions with their corresponding identifiers and whether or not they are currently alive. Embedding \u00b6 You can embed the browser recording directly into your existing tools by using the JavaScript SDK. You need to import the script in your <head> and add a custom HTML tag to the body where the video should be placed. < head > < script src = \"http://<your-webgrid-address>/embed\" defer ></ script > </ head > < body > < webgrid-video session-id = \"<your-session-id>\" ></ webgrid-video > </ body > By default, the script tries to guess the webgrid address from the script tag. This behaviour can be overruled by adding the host=\"<your-webgrid-address\" attribute to the video tag. Especially when fetching the script for a static page builder which embeds it directly, this is required as the host is evaluated at runtime not at request time. Viewing \u00b6 If you want to monitor your session manually you may use the video player provided by the grid. To do so just plug your session id from above into this url: http://<your-webgrid-address>/embed/<session-id> Danger This player is not supposed to be embedded into other pages through iframes or other means. For embedding refer the embedding section!","title":"Screen recording"},{"location":"features/screen-recording/#screen-recording","text":"The grid is capable of capturing the screen for each browser session you create. This feature is enabled by default in a local Docker instance, but requires some additional configuration in a cluster. Warning By default, this feature is disabled in the Kubernetes helm chart as it requires additional setup. Details on how to get up and running can be found on the Kubernetes storage page !","title":"Screen recording"},{"location":"features/screen-recording/#session-id","text":"In order to view or embed a video you need to retrieve its unique session identifier. The simplest method is through your client library \u2014 below are a few examples: Java FirefoxOptions firefoxOptions = new FirefoxOptions (); WebDriver driver = new RemoteWebDriver ( new URL ( \"http://localhost\" ), firefoxOptions ); driver . get ( \"http://dcuk.com\" ); SessionId session = (( FirefoxDriver ) driver ). getSessionId (); System . out . println ( \"Session id: \" + session . toString ()); System . in . read (); driver . quit (); Python from selenium import webdriver firefox_options = webdriver . FirefoxOptions () driver = webdriver . Remote ( command_executor = 'http://localhost' , options = firefox_options ) driver . get ( \"http://www.google.com\" ) print ( \"Session id: \" + driver . session_id ); input ( \"Press Enter to quit...\" ) driver . quit () Retrieving session ID from API Alternatively, you can get a list of all (or just active) sessions from the grid API. Note however that the list can be very large, so it might be hard to identify yours. Use the following query to get a list of sessions e.g. by pasting it into the API explorer or using a GraphQL client: query { sessions { id alive } } It returns you a list of all sessions with their corresponding identifiers and whether or not they are currently alive.","title":"Session ID"},{"location":"features/screen-recording/#embedding","text":"You can embed the browser recording directly into your existing tools by using the JavaScript SDK. You need to import the script in your <head> and add a custom HTML tag to the body where the video should be placed. < head > < script src = \"http://<your-webgrid-address>/embed\" defer ></ script > </ head > < body > < webgrid-video session-id = \"<your-session-id>\" ></ webgrid-video > </ body > By default, the script tries to guess the webgrid address from the script tag. This behaviour can be overruled by adding the host=\"<your-webgrid-address\" attribute to the video tag. Especially when fetching the script for a static page builder which embeds it directly, this is required as the host is evaluated at runtime not at request time.","title":"Embedding"},{"location":"features/screen-recording/#viewing","text":"If you want to monitor your session manually you may use the video player provided by the grid. To do so just plug your session id from above into this url: http://<your-webgrid-address>/embed/<session-id> Danger This player is not supposed to be embedded into other pages through iframes or other means. For embedding refer the embedding section!","title":"Viewing"},{"location":"kubernetes/access/","text":"Accessing the grid \u00b6 By default the chart does not create an Ingress object for the Service. This means that the grid will only be accessible from within your cluster. You can use any Kubernetes mechanic to expose it to your clients \u2014 below are some examples to point you in the right direction. Note If you changed the name or namespace of the release during installation you have to adjust it accordingly in the examples below! Cluster internal access \u00b6 If your Selenium clients (e.g. test suites) are running inside your cluster you can directly interact with the Service created by the helm chart. In the default Kubernetes setup you can access it through either http://example-webgrid within the same namespace or http://example-webgrid.namespace from a different one. Port forwarding \u00b6 In case you have a local client and want to use the grid you can temporarily forward a local port to the Service in the cluster. Below is an example that would expose the grid at http://localhost:3030 . kubectl port-forward service/example-web-grid 3030:80 Ingress object \u00b6 If you want to access the grid from outside the cluster you can use any method Kubernetes provides to expose the Service. Below is an example configuration object. apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : webgrid spec : rules : - host : webgrid.your-host.dev http : paths : - path : / backend : serviceName : example-web-grid servicePort : http Note You may have to add additional properties to the spec for it to work depending on your cluster setup. Consult your cluster admin (or documentation) for more details. Warning As the Ingress adds another HTTP proxy to the request chain it could pose a bottleneck! If you want to avoid this consider running your tests inside the cluster .","title":"Accessing the grid"},{"location":"kubernetes/access/#accessing-the-grid","text":"By default the chart does not create an Ingress object for the Service. This means that the grid will only be accessible from within your cluster. You can use any Kubernetes mechanic to expose it to your clients \u2014 below are some examples to point you in the right direction. Note If you changed the name or namespace of the release during installation you have to adjust it accordingly in the examples below!","title":"Accessing the grid"},{"location":"kubernetes/access/#cluster-internal-access","text":"If your Selenium clients (e.g. test suites) are running inside your cluster you can directly interact with the Service created by the helm chart. In the default Kubernetes setup you can access it through either http://example-webgrid within the same namespace or http://example-webgrid.namespace from a different one.","title":"Cluster internal access"},{"location":"kubernetes/access/#port-forwarding","text":"In case you have a local client and want to use the grid you can temporarily forward a local port to the Service in the cluster. Below is an example that would expose the grid at http://localhost:3030 . kubectl port-forward service/example-web-grid 3030:80","title":"Port forwarding"},{"location":"kubernetes/access/#ingress-object","text":"If you want to access the grid from outside the cluster you can use any method Kubernetes provides to expose the Service. Below is an example configuration object. apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : webgrid spec : rules : - host : webgrid.your-host.dev http : paths : - path : / backend : serviceName : example-web-grid servicePort : http Note You may have to add additional properties to the spec for it to work depending on your cluster setup. Consult your cluster admin (or documentation) for more details. Warning As the Ingress adds another HTTP proxy to the request chain it could pose a bottleneck! If you want to avoid this consider running your tests inside the cluster .","title":"Ingress object"},{"location":"kubernetes/configuration/","text":"Configuration \u00b6 To get you started as quickly as possible the helm chart uses a set of default values that work in most clusters. However, these defaults are only an entrypoint \u2014 it is very likely that these need to be adapted to your specific needs. Other documentation topics may ask you to change values in order to enable advanced features like screen recordings. Refer the sections below to learn how to do so. Changing the defaults \u00b6 To override values you should create a file called webgrid-chart-values.yaml which contains the settings you want to change and apply it using helm: # During installation helm install -f webgrid-chart-values.yaml example webgrid/webgrid # Change a running grid helm upgrade -f webgrid-chart-values.yaml example webgrid/webgrid Note If you changed the name or namespace of the release during installation you have to adjust it accordingly in the example commands above! Value reference \u00b6 Below is a reference of all default helm values with their documentations. You can also find those in the source code of the chart. Default helm values # Default values for web - grid . # This is a YAML - formatted file . # Declare variables to be passed into your templates . logLevel: debug , hyper = warn , warp = warn maxSessionsPerOrchestrator: 5 recording: # Whether or not to create PVCs and a storage service enabled : false sizeLimit : 50 G # Consult the ffmpeg documentation on how these parameters work # https : //trac.ffmpeg.org/wiki/Encode/H.264 quality : crf : 46 maxBitrate : 450000 persistentVolumeClaim : create : true # The name of the volume claim to use . # If not set and createVolumeClaim is true , a name is generated using the fullname template name : \"\" # A backing PersistentVolume has to exist and implicitly defines # the node - affinity for sessions and storage pods if recording is enabled . # # Note : The storage class needs to support concurrent access by multiple pods ! ( e . g . a hostPath based PV ) storageClassName : local - storage replicaCount: proxy : 1 manager : 1 orchestrator : 1 metrics : 1 api : 1 image: repository : webgrid pullPolicy : Always serviceAccount: # Specifies whether a service account with RBAC should be created create : true # Annotations to add to the service account annotations : {} # The name of the service account to use . # If not set and create is true , a name is generated using the fullname template name : \"\" storageClassName: redis : emptyDir resources: redis : {} proxy : {} manager : {} metrics : {} orchestrator : {} storage : {} api : {} session : limits : memory : 3000 Mi requests : cpu : '1' memory : 3000 Mi nodeSelector: redis : {} proxy : {} manager : {} metrics : {} orchestrator : {} session : {} storage : {} api : {} tolerations: redis : [] proxy : [] manager : [] metrics : [] orchestrator : [] session : [] storage : [] api : [] affinity: redis : {} proxy : {} manager : {} metrics : {} orchestrator : {} session : {} storage : {} api : {} service: type : ClusterIP port : 80 Todo Sorry for the broken syntax highlighting. The plugin needs to be updated \u2014 tracking PR: rnorth/mkdocs-codeinclude-plugin#5","title":"Configuration"},{"location":"kubernetes/configuration/#configuration","text":"To get you started as quickly as possible the helm chart uses a set of default values that work in most clusters. However, these defaults are only an entrypoint \u2014 it is very likely that these need to be adapted to your specific needs. Other documentation topics may ask you to change values in order to enable advanced features like screen recordings. Refer the sections below to learn how to do so.","title":"Configuration"},{"location":"kubernetes/configuration/#changing-the-defaults","text":"To override values you should create a file called webgrid-chart-values.yaml which contains the settings you want to change and apply it using helm: # During installation helm install -f webgrid-chart-values.yaml example webgrid/webgrid # Change a running grid helm upgrade -f webgrid-chart-values.yaml example webgrid/webgrid Note If you changed the name or namespace of the release during installation you have to adjust it accordingly in the example commands above!","title":"Changing the defaults"},{"location":"kubernetes/configuration/#value-reference","text":"Below is a reference of all default helm values with their documentations. You can also find those in the source code of the chart. Default helm values # Default values for web - grid . # This is a YAML - formatted file . # Declare variables to be passed into your templates . logLevel: debug , hyper = warn , warp = warn maxSessionsPerOrchestrator: 5 recording: # Whether or not to create PVCs and a storage service enabled : false sizeLimit : 50 G # Consult the ffmpeg documentation on how these parameters work # https : //trac.ffmpeg.org/wiki/Encode/H.264 quality : crf : 46 maxBitrate : 450000 persistentVolumeClaim : create : true # The name of the volume claim to use . # If not set and createVolumeClaim is true , a name is generated using the fullname template name : \"\" # A backing PersistentVolume has to exist and implicitly defines # the node - affinity for sessions and storage pods if recording is enabled . # # Note : The storage class needs to support concurrent access by multiple pods ! ( e . g . a hostPath based PV ) storageClassName : local - storage replicaCount: proxy : 1 manager : 1 orchestrator : 1 metrics : 1 api : 1 image: repository : webgrid pullPolicy : Always serviceAccount: # Specifies whether a service account with RBAC should be created create : true # Annotations to add to the service account annotations : {} # The name of the service account to use . # If not set and create is true , a name is generated using the fullname template name : \"\" storageClassName: redis : emptyDir resources: redis : {} proxy : {} manager : {} metrics : {} orchestrator : {} storage : {} api : {} session : limits : memory : 3000 Mi requests : cpu : '1' memory : 3000 Mi nodeSelector: redis : {} proxy : {} manager : {} metrics : {} orchestrator : {} session : {} storage : {} api : {} tolerations: redis : [] proxy : [] manager : [] metrics : [] orchestrator : [] session : [] storage : [] api : [] affinity: redis : {} proxy : {} manager : {} metrics : {} orchestrator : {} session : {} storage : {} api : {} service: type : ClusterIP port : 80 Todo Sorry for the broken syntax highlighting. The plugin needs to be updated \u2014 tracking PR: rnorth/mkdocs-codeinclude-plugin#5","title":"Value reference"},{"location":"kubernetes/scaling/","text":"Scaling \u00b6 As the grid is optimized for performance, it should work for most small-scale scenarios. However, if you are running into performance issues it can be scaled up to meet demand. The grid is split into multiple distinct components and each can be replicated individually (apart from the database \u2014 but it should outperform everything else by a long stretch ). You should look at which component requires scaling. Below is a list of common scenarios where bottlenecks are expected. Concurrent sessions \u00b6 By default, the number of concurrent sessions is limited to five per orchestrator and one Kubernetes orchestrator. Normally, only one orchestrator is required even for very large setups so the per-orchestrator limit should be used. To change the number of concurrent sessions allowed merge and apply the following helm value as described here : maxSessionsPerOrchestrator : 5 Traffic congestion \u00b6 Another common bottleneck, which is especially common with regular Selenium Grids, is the proxy server. Due to protocol constraints all traffic has to be routed through an intermediate instance, which inherently creates a choke point. This can be remedied by the microservice architecture of WebGrid. To increase the number of proxy servers that route session control traffic, merge and apply the following helm value as described here : replicaCount : proxy : 2","title":"Scaling"},{"location":"kubernetes/scaling/#scaling","text":"As the grid is optimized for performance, it should work for most small-scale scenarios. However, if you are running into performance issues it can be scaled up to meet demand. The grid is split into multiple distinct components and each can be replicated individually (apart from the database \u2014 but it should outperform everything else by a long stretch ). You should look at which component requires scaling. Below is a list of common scenarios where bottlenecks are expected.","title":"Scaling"},{"location":"kubernetes/scaling/#concurrent-sessions","text":"By default, the number of concurrent sessions is limited to five per orchestrator and one Kubernetes orchestrator. Normally, only one orchestrator is required even for very large setups so the per-orchestrator limit should be used. To change the number of concurrent sessions allowed merge and apply the following helm value as described here : maxSessionsPerOrchestrator : 5","title":"Concurrent sessions"},{"location":"kubernetes/scaling/#traffic-congestion","text":"Another common bottleneck, which is especially common with regular Selenium Grids, is the proxy server. Due to protocol constraints all traffic has to be routed through an intermediate instance, which inherently creates a choke point. This can be remedied by the microservice architecture of WebGrid. To increase the number of proxy servers that route session control traffic, merge and apply the following helm value as described here : replicaCount : proxy : 2","title":"Traffic congestion"},{"location":"kubernetes/storage/","text":"Grid storage \u00b6 Some features, like screen recording, require a persistent storage. Due to the vast possibilities to manage storage, no standard values are given and it is disabled by default. To enable it, merge and apply the following helm values as described here : recording : enabled : true Limiting the storage size By default the grid uses at most 50GB of storage for each volume and deletes old videos to keep the occupancy below that. You may change this value by adding the recording.sizeLimit value with a Kubernetes compatible string like 50G . For more details, refer the configuration defaults . Persistent Volume \u00b6 To store the files a persistent volume is required. Due to the architecture of the grid the volume has to be mountable into multiple pods simultaneously! One way of achieving this is to allocate some storage on each worker node, however you may use any method your cluster allows. Local storage \u00b6 The simplest method is a directory on each worker node used for the grid. Here is an example of a PersistentVolume: apiVersion : v1 kind : PersistentVolume metadata : name : webgrid-pv spec : capacity : storage : 50Gi volumeMode : Filesystem accessModes : - ReadWriteOnce persistentVolumeReclaimPolicy : Delete storageClassName : local-storage local : path : <storage-path-on-your-nodes> You may want to add a node affinity to constrain it to nodes you plan on using with webgrid. To finish the setup, set the storage class as described below to local-storage . Kubernetes bug workaround During development, we encountered a bug which prevents a reuse of the PersistentVolume. When binding to the PV once and then re-binding later with another volume claim, the volume enters a blocked state which can only be resolved manually. To work around this, the chart includes the possibility to re-use an existing PVC for all tasks instead of creating its own. To enable this feature, manually create a volume claim like this: apiVersion : v1 kind : PersistentVolumeClaim metadata : name : <your-pvc-name> spec : storageClassName : local-storage accessModes : - ReadWriteOnce resources : requests : storage : 50Gi Then, merge and apply the following helm values as described here recording : persistentVolumeClaim : create : false name : \"<your-pvc-name>\" Volume claim \u00b6 By default, the helm chart creates its own PersistentVolumeClaim. To set the storage class, merge and apply the following helm values as described here : recording : persistentVolumeClaim : storageClassName : \"<your-storage-class>\"","title":"Grid storage"},{"location":"kubernetes/storage/#grid-storage","text":"Some features, like screen recording, require a persistent storage. Due to the vast possibilities to manage storage, no standard values are given and it is disabled by default. To enable it, merge and apply the following helm values as described here : recording : enabled : true Limiting the storage size By default the grid uses at most 50GB of storage for each volume and deletes old videos to keep the occupancy below that. You may change this value by adding the recording.sizeLimit value with a Kubernetes compatible string like 50G . For more details, refer the configuration defaults .","title":"Grid storage"},{"location":"kubernetes/storage/#persistent-volume","text":"To store the files a persistent volume is required. Due to the architecture of the grid the volume has to be mountable into multiple pods simultaneously! One way of achieving this is to allocate some storage on each worker node, however you may use any method your cluster allows.","title":"Persistent Volume"},{"location":"kubernetes/storage/#local-storage","text":"The simplest method is a directory on each worker node used for the grid. Here is an example of a PersistentVolume: apiVersion : v1 kind : PersistentVolume metadata : name : webgrid-pv spec : capacity : storage : 50Gi volumeMode : Filesystem accessModes : - ReadWriteOnce persistentVolumeReclaimPolicy : Delete storageClassName : local-storage local : path : <storage-path-on-your-nodes> You may want to add a node affinity to constrain it to nodes you plan on using with webgrid. To finish the setup, set the storage class as described below to local-storage . Kubernetes bug workaround During development, we encountered a bug which prevents a reuse of the PersistentVolume. When binding to the PV once and then re-binding later with another volume claim, the volume enters a blocked state which can only be resolved manually. To work around this, the chart includes the possibility to re-use an existing PVC for all tasks instead of creating its own. To enable this feature, manually create a volume claim like this: apiVersion : v1 kind : PersistentVolumeClaim metadata : name : <your-pvc-name> spec : storageClassName : local-storage accessModes : - ReadWriteOnce resources : requests : storage : 50Gi Then, merge and apply the following helm values as described here recording : persistentVolumeClaim : create : false name : \"<your-pvc-name>\"","title":"Local storage"},{"location":"kubernetes/storage/#volume-claim","text":"By default, the helm chart creates its own PersistentVolumeClaim. To set the storage class, merge and apply the following helm values as described here : recording : persistentVolumeClaim : storageClassName : \"<your-storage-class>\"","title":"Volume claim"}]}