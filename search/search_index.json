{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WebGrid Install | Usage | Docs You have an idea for a logo? Submit it here! Cluster ready. Designed with concurrency and on-demand scalability 1 in mind Debuggable. Provides browser screen recordings, extensive logs, and tracing Fast. Built for speed and performance on a single grid instance W3C Specification compilant. Fully compatible with existing Selenium 4 clients 1 All the way down to zero, obviously Install \u00b6 Below are quick-start tutorials to get you started. For a more detailed introduction visit the dedicated Getting Started guide ! \ud83d\udc33 Docker \u00b6 To run a basic grid in Docker you can use Docker Compose. Below is a bare-bones example of getting all required components up and running! # Create prerequisites docker network create webgrid # Download compose file curl -fsSLO webgrid.dev/docker-compose.yml # Launch the grid docker-compose up You can now point your Selenium client to localhost:8080 and browse the API at /api . \u2638\ufe0f Kube \u00b6 For deployment to Kubernetes a Helm repository is available. The default values provide a good starting point for basic cluster setups like K3s or microk8s . # Add the repository helm repo add webgrid https://webgrid.dev/ # List all available versions helm search repo --versions --devel webgrid/demo # Install the chart helm install example webgrid/demo --version \"<pick-a-version-from-the-list>\" # Make it accessible locally for evaluation kubectl port-forward service/example-webgrid 8080 :80 Your grid is now available at localhost:8080 . If you are deploying to a RBAC enabled cluster you might have to tweak some settings. Take a look at the documentation on how to use your own ServiceAccount and PersistentVolumeClaims. Usage \u00b6 Once you have your grid up and running there is a couple of things you can do! \ud83d\ude80 Launch browser instances \u00b6 Point your selenium client to http://localhost:8080 to create a new browser container/pod and interact with it! You can use all features supported by Selenium. \ud83d\udd0d Browse the API \u00b6 The grid provides a GraphQL API at /api with a Playground for you to explore. It exposes all available metadata about sessions, grid health and advanced features like video recordings. \ud83d\udcfa Watch your browsers \u00b6 You can take a live look at what your browsers are doing by taking the Session ID of a instance and visiting localhost:8080 . You can also embed the videos in your existing tools! Head over to the embedding documentation to learn how. Screen recordings in clusters Video recordings are disabled by default in K8s as every cluster has specific requirements for file storage. The storage documentation explains how to enable it. Developing \u00b6 If you want to build the project locally you can use the Makefile . To create Docker images for every component and run them locally run these commands: # Build docker images make # Start components in docker make install To start individual components outside of Docker or setup the development environment, see the development environment documentation . License \u00b6 This project is licensed under the MIT License. While this does grant you a lot of freedom in how to use the software and keeps the legal headache to a minimum, it also no longer requires you to publish modifications made to the project. The original intention behind the AGPL license was to encourage contributions by users who added features for their own use. Since this project is so small at this stage, it heavily relies on feedback and contributions from the community (thats you!). So please strongly consider contributing any changes you make for the benefit of all users \ud83d\ude42","title":"Index"},{"location":"#install","text":"Below are quick-start tutorials to get you started. For a more detailed introduction visit the dedicated Getting Started guide !","title":"Install"},{"location":"#docker","text":"To run a basic grid in Docker you can use Docker Compose. Below is a bare-bones example of getting all required components up and running! # Create prerequisites docker network create webgrid # Download compose file curl -fsSLO webgrid.dev/docker-compose.yml # Launch the grid docker-compose up You can now point your Selenium client to localhost:8080 and browse the API at /api .","title":"\ud83d\udc33 Docker"},{"location":"#kube","text":"For deployment to Kubernetes a Helm repository is available. The default values provide a good starting point for basic cluster setups like K3s or microk8s . # Add the repository helm repo add webgrid https://webgrid.dev/ # List all available versions helm search repo --versions --devel webgrid/demo # Install the chart helm install example webgrid/demo --version \"<pick-a-version-from-the-list>\" # Make it accessible locally for evaluation kubectl port-forward service/example-webgrid 8080 :80 Your grid is now available at localhost:8080 . If you are deploying to a RBAC enabled cluster you might have to tweak some settings. Take a look at the documentation on how to use your own ServiceAccount and PersistentVolumeClaims.","title":"\u2638\ufe0f Kube"},{"location":"#usage","text":"Once you have your grid up and running there is a couple of things you can do!","title":"Usage"},{"location":"#launch-browser-instances","text":"Point your selenium client to http://localhost:8080 to create a new browser container/pod and interact with it! You can use all features supported by Selenium.","title":"\ud83d\ude80 Launch browser instances"},{"location":"#browse-the-api","text":"The grid provides a GraphQL API at /api with a Playground for you to explore. It exposes all available metadata about sessions, grid health and advanced features like video recordings.","title":"\ud83d\udd0d Browse the API"},{"location":"#watch-your-browsers","text":"You can take a live look at what your browsers are doing by taking the Session ID of a instance and visiting localhost:8080 . You can also embed the videos in your existing tools! Head over to the embedding documentation to learn how. Screen recordings in clusters Video recordings are disabled by default in K8s as every cluster has specific requirements for file storage. The storage documentation explains how to enable it.","title":"\ud83d\udcfa Watch your browsers"},{"location":"#developing","text":"If you want to build the project locally you can use the Makefile . To create Docker images for every component and run them locally run these commands: # Build docker images make # Start components in docker make install To start individual components outside of Docker or setup the development environment, see the development environment documentation .","title":"Developing"},{"location":"#license","text":"This project is licensed under the MIT License. While this does grant you a lot of freedom in how to use the software and keeps the legal headache to a minimum, it also no longer requires you to publish modifications made to the project. The original intention behind the AGPL license was to encourage contributions by users who added features for their own use. Since this project is so small at this stage, it heavily relies on feedback and contributions from the community (thats you!). So please strongly consider contributing any changes you make for the benefit of all users \ud83d\ude42","title":"License"},{"location":"faq/","text":"FAQ \u00b6 About this project \u00b6 Why does this exist? \u00b6 As of early 2020 only a handful solutions for distributed selenium based software tests existed. Most of these did not provide support for advanced features like dynamic scaling/provisioning and efficient video recordings with additional problems like thread safety issues and poor scaling for large scale applications with hundreds of browsers. Selenium Grid Selenium Grid has been in use at my current workplace for some years and worked acceptable. However, with projects growing in both test volume and manpower the amount of concurrent browsers grew rather quickly. This exposed known bottlenecks in the single-proxy design of the Selenium Grid's architecture (which yielded projects like GridRouter which try to work around the fundamental design problem). Additionally, it raised concerns about the constant resource usage due to the static allocation of nodes creating a requirement for dynamic allocation. Add to this the fact that Grid 3 has no official support anymore and Grid 4 has been in development for about two years now with no release date in sight (Grid 4 also lacks features like dynamic scalability and screen recordings)! Zalenium Zalando ran into similar findings and created an extension to the regular Selenium Grid called Zalenium . It boasts features like a dashboard with VNC viewers and screen recordings. However, it had its own fair share of issues on top of the ones that the regular grid exposed, yielding even worse test flakiness on a daily basis. Zalando stopped maintaining the project as of early 2020. Commercial solutions Aerokube provides a commercial off-the-shelve solution for scalable selenium grids in Kubernetes. However, for our application the pricing philosophy was out of reach by a long stretch and it was cheaper to set aside some development resources to create this Open Source solution with the added benefit of making scalable Grids available to the community! This lead to a investigation of our options to continue Selenium tests with dynamic scaling, screen recording and other future additions on the wish-list. As the underlying protocol of Selenium has been standardized by the W3C we reached the conclusion that it was feasible to develop our own solution to this problem within a few months time. With that this project was born in April 2020. Who is behind this? \u00b6 The project originates from an internal requirement at PPI AG . However, as the project progressed it became clear that other people could greatly benefit from it. I have personally taken over the public development and maintenance of the project on GitHub and it is no longer directly affiliated with the PPI AG nor does the company provide any kind of support or responsibility! I am a software engineer from Germany who recently graduated from the Nordakademie and currently works at PPI AG. Why are there so few issues in the tracker? \u00b6 The project has been developed internally at first using a private GitLab instance. Later, the decision to go public has been made and everything except past issues has been moved over. Technical details \u00b6 Does it work with Selenium? \u00b6 Yes it does, thanks to the standardization of the underlying protocol by the W3C. See below . How well does it scale \u00b6 Let's just say that we haven't reached any performance limits yet in our internal testing with a few hundred browser instances. In theory, the only limit is your cluster bandwidth and to some degree the performance of the Redis database server (although this only affects session creation and not running browser sessions). What is this sorcery?! \ud83e\uddd9\u200d\u2642\ufe0f \u00b6 No magic, just standardization and an efficient architecture \ud83d\ude09 WebGrid relies on the WebDriver specification. When you execute a Selenium Test, you are effectively speaking to an implementation of this protocol. Almost all browser vendors provide such an API for their browser. Both Selenium Grid and WebGrid are just intermediates who delegate your requests to a browser. The standardization allows us to implement a completely transparent solution which looks just like your local browser or Selenium Grid. Furthermore, by employing a modern architecture, which is built for stability and performance with clusters and dynamic deployment of browsers in mind, we can provide significantly better scalability and speed! If you have any questions about the inner workings don't hesitate to ask either on GitHub Discussions , the official Discord server , or by contacting me via mail ! Where is the latest tag? \u00b6 We believe that the :latest in Docker is very evil and dangerous . Apart from the problem that it is just yet another tag and everything else being purely convention, you should make a conscious choice to upgrade your production environment from one version to another. For this reason we are releasing versions on all distribution platforms following the SemVer 2.0 convention so you can know which versions are safe and which might require some more work. Additionally, all Helm charts and Compose files use pinned versions for the Docker images.","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#about-this-project","text":"","title":"About this project"},{"location":"faq/#why-does-this-exist","text":"As of early 2020 only a handful solutions for distributed selenium based software tests existed. Most of these did not provide support for advanced features like dynamic scaling/provisioning and efficient video recordings with additional problems like thread safety issues and poor scaling for large scale applications with hundreds of browsers. Selenium Grid Selenium Grid has been in use at my current workplace for some years and worked acceptable. However, with projects growing in both test volume and manpower the amount of concurrent browsers grew rather quickly. This exposed known bottlenecks in the single-proxy design of the Selenium Grid's architecture (which yielded projects like GridRouter which try to work around the fundamental design problem). Additionally, it raised concerns about the constant resource usage due to the static allocation of nodes creating a requirement for dynamic allocation. Add to this the fact that Grid 3 has no official support anymore and Grid 4 has been in development for about two years now with no release date in sight (Grid 4 also lacks features like dynamic scalability and screen recordings)! Zalenium Zalando ran into similar findings and created an extension to the regular Selenium Grid called Zalenium . It boasts features like a dashboard with VNC viewers and screen recordings. However, it had its own fair share of issues on top of the ones that the regular grid exposed, yielding even worse test flakiness on a daily basis. Zalando stopped maintaining the project as of early 2020. Commercial solutions Aerokube provides a commercial off-the-shelve solution for scalable selenium grids in Kubernetes. However, for our application the pricing philosophy was out of reach by a long stretch and it was cheaper to set aside some development resources to create this Open Source solution with the added benefit of making scalable Grids available to the community! This lead to a investigation of our options to continue Selenium tests with dynamic scaling, screen recording and other future additions on the wish-list. As the underlying protocol of Selenium has been standardized by the W3C we reached the conclusion that it was feasible to develop our own solution to this problem within a few months time. With that this project was born in April 2020.","title":"Why does this exist?"},{"location":"faq/#who-is-behind-this","text":"The project originates from an internal requirement at PPI AG . However, as the project progressed it became clear that other people could greatly benefit from it. I have personally taken over the public development and maintenance of the project on GitHub and it is no longer directly affiliated with the PPI AG nor does the company provide any kind of support or responsibility! I am a software engineer from Germany who recently graduated from the Nordakademie and currently works at PPI AG.","title":"Who is behind this?"},{"location":"faq/#why-are-there-so-few-issues-in-the-tracker","text":"The project has been developed internally at first using a private GitLab instance. Later, the decision to go public has been made and everything except past issues has been moved over.","title":"Why are there so few issues in the tracker?"},{"location":"faq/#technical-details","text":"","title":"Technical details"},{"location":"faq/#does-it-work-with-selenium","text":"Yes it does, thanks to the standardization of the underlying protocol by the W3C. See below .","title":"Does it work with Selenium?"},{"location":"faq/#how-well-does-it-scale","text":"Let's just say that we haven't reached any performance limits yet in our internal testing with a few hundred browser instances. In theory, the only limit is your cluster bandwidth and to some degree the performance of the Redis database server (although this only affects session creation and not running browser sessions).","title":"How well does it scale"},{"location":"faq/#what-is-this-sorcery","text":"No magic, just standardization and an efficient architecture \ud83d\ude09 WebGrid relies on the WebDriver specification. When you execute a Selenium Test, you are effectively speaking to an implementation of this protocol. Almost all browser vendors provide such an API for their browser. Both Selenium Grid and WebGrid are just intermediates who delegate your requests to a browser. The standardization allows us to implement a completely transparent solution which looks just like your local browser or Selenium Grid. Furthermore, by employing a modern architecture, which is built for stability and performance with clusters and dynamic deployment of browsers in mind, we can provide significantly better scalability and speed! If you have any questions about the inner workings don't hesitate to ask either on GitHub Discussions , the official Discord server , or by contacting me via mail !","title":"What is this sorcery?! \ud83e\uddd9\u200d\u2642\ufe0f"},{"location":"faq/#where-is-the-latest-tag","text":"We believe that the :latest in Docker is very evil and dangerous . Apart from the problem that it is just yet another tag and everything else being purely convention, you should make a conscious choice to upgrade your production environment from one version to another. For this reason we are releasing versions on all distribution platforms following the SemVer 2.0 convention so you can know which versions are safe and which might require some more work. Additionally, all Helm charts and Compose files use pinned versions for the Docker images.","title":"Where is the latest tag?"},{"location":"getting-started/","text":"Getting started \u00b6 Below are guides to get you started as quickly as possible on your specific platform! They provide a set of sane defaults which can later be tweaked for your use-case. Docker \u00b6 If you want to run a grid locally for simple testing purposes that require multiple isolated browsers or just want to evaluate this tool this is the right choice. Make sure you have Docker installed and configured properly. Before you can start the grid in Docker you have to create a network for it: docker network create webgrid Next you need to download the latest docker-compose.yml and run it: # Download compose file curl -fsSLO https://webgrid.dev/docker-compose.yml # Launch the grid docker-compose up Continue reading below on how to send requests to your grid. Kubernetes \u00b6 WebGrid provides a Helm chart to get started as quickly as possible. Below is a guide on how to add the chart repository and install the chart. You can change the name of the release in the second command or add other options like the target namespace \u2014 for more details consult the Helm documentation. # Add the repository helm repo add webgrid https://webgrid.dev/ # List all available versions helm search repo --versions --devel webgrid/demo # Install the chart helm install example webgrid/demo --version \"<pick-a-version-from-the-list>\" Using the grid \u00b6 Once you have started the grid you can send requests to it using the regular Selenium client libraries available here . Java FirefoxOptions firefoxOptions = new FirefoxOptions (); WebDriver driver = new RemoteWebDriver ( new URL ( \"http://localhost:8080\" ), firefoxOptions ); driver . get ( \"http://www.google.com\" ); driver . quit (); Python from selenium import webdriver firefox_options = webdriver . FirefoxOptions () driver = webdriver . Remote ( command_executor = 'http://localhost:8080' , options = firefox_options ) driver . get ( \"http://www.google.com\" ) driver . quit () C# FirefoxOptions firefoxOptions = new FirefoxOptions (); IWebDriver driver = new RemoteWebDriver ( new Uri ( \"http://localhost:8080\" ), firefoxOptions ); driver . Navigate (). GoToUrl ( \"http://www.google.com\" ); driver . Quit (); Ruby require 'selenium-webdriver' driver = Selenium :: WebDriver . for :remote , url : \"http://localhost:8080\" , desired_capabilities : :firefox driver . get \"http://www.google.com\" driver . close JavaScript const { Builder , Capabilities } = require ( \"selenium-webdriver\" ); var capabilities = Capabilities . firefox (); ( async function helloSelenium () { let driver = new Builder () . usingServer ( \"http://localhost:8080\" ) . withCapabilities ( capabilities ) . build (); try { await driver . get ( 'http://www.google.com' ); } finally { await driver . quit (); } })(); Kotlin firefoxOptions = FirefoxOptions () driver : WebDriver = new RemoteWebDriver ( new URL ( \"http://localhost:8080\" ), firefoxOptions ) driver . get ( \"http://www.google.com\" ) driver . quit () Attention When you used Kubernetes you may have to forward the grid service to your local computer for the example code to work. For details on accessing your WebGrid within a cluster consult the Kubernetes specific docs.","title":"Getting started"},{"location":"getting-started/#getting-started","text":"Below are guides to get you started as quickly as possible on your specific platform! They provide a set of sane defaults which can later be tweaked for your use-case.","title":"Getting started"},{"location":"getting-started/#docker","text":"If you want to run a grid locally for simple testing purposes that require multiple isolated browsers or just want to evaluate this tool this is the right choice. Make sure you have Docker installed and configured properly. Before you can start the grid in Docker you have to create a network for it: docker network create webgrid Next you need to download the latest docker-compose.yml and run it: # Download compose file curl -fsSLO https://webgrid.dev/docker-compose.yml # Launch the grid docker-compose up Continue reading below on how to send requests to your grid.","title":"Docker"},{"location":"getting-started/#kubernetes","text":"WebGrid provides a Helm chart to get started as quickly as possible. Below is a guide on how to add the chart repository and install the chart. You can change the name of the release in the second command or add other options like the target namespace \u2014 for more details consult the Helm documentation. # Add the repository helm repo add webgrid https://webgrid.dev/ # List all available versions helm search repo --versions --devel webgrid/demo # Install the chart helm install example webgrid/demo --version \"<pick-a-version-from-the-list>\"","title":"Kubernetes"},{"location":"getting-started/#using-the-grid","text":"Once you have started the grid you can send requests to it using the regular Selenium client libraries available here . Java FirefoxOptions firefoxOptions = new FirefoxOptions (); WebDriver driver = new RemoteWebDriver ( new URL ( \"http://localhost:8080\" ), firefoxOptions ); driver . get ( \"http://www.google.com\" ); driver . quit (); Python from selenium import webdriver firefox_options = webdriver . FirefoxOptions () driver = webdriver . Remote ( command_executor = 'http://localhost:8080' , options = firefox_options ) driver . get ( \"http://www.google.com\" ) driver . quit () C# FirefoxOptions firefoxOptions = new FirefoxOptions (); IWebDriver driver = new RemoteWebDriver ( new Uri ( \"http://localhost:8080\" ), firefoxOptions ); driver . Navigate (). GoToUrl ( \"http://www.google.com\" ); driver . Quit (); Ruby require 'selenium-webdriver' driver = Selenium :: WebDriver . for :remote , url : \"http://localhost:8080\" , desired_capabilities : :firefox driver . get \"http://www.google.com\" driver . close JavaScript const { Builder , Capabilities } = require ( \"selenium-webdriver\" ); var capabilities = Capabilities . firefox (); ( async function helloSelenium () { let driver = new Builder () . usingServer ( \"http://localhost:8080\" ) . withCapabilities ( capabilities ) . build (); try { await driver . get ( 'http://www.google.com' ); } finally { await driver . quit (); } })(); Kotlin firefoxOptions = FirefoxOptions () driver : WebDriver = new RemoteWebDriver ( new URL ( \"http://localhost:8080\" ), firefoxOptions ) driver . get ( \"http://www.google.com\" ) driver . quit () Attention When you used Kubernetes you may have to forward the grid service to your local computer for the example code to work. For details on accessing your WebGrid within a cluster consult the Kubernetes specific docs.","title":"Using the grid"},{"location":"architecture/","text":"Overview \u00b6 No overview of the architecture exists yet :( While the following pages contain some information, large portions of it are outdated due to the recent architecture overhaul. It will take me a bit to write about the new architecture. In the meantime, you can take a look at the mostly documented code for the core services over here .","title":"Overview"},{"location":"architecture/#overview","text":"No overview of the architecture exists yet :( While the following pages contain some information, large portions of it are outdated due to the recent architecture overhaul. It will take me a bit to write about the new architecture. In the meantime, you can take a look at the mostly documented code for the core services over here .","title":"Overview"},{"location":"architecture/database/","text":"Database \u00b6 Outdated Due to a recent architectural overhaul, the information on this page is no longer applicable for versions beyond v0.5.1-beta . Keys \u00b6 All metadata is stored in a key-value in-memory database called Redis . Below is a list of all keys that are currently in use, annotated with their type and format (if applicable). Root lists \u00b6 `orchestrators` = Set < string > // uuids `sessions.active` = Set < string > // uuids `sessions.terminated` = Set < string > // uuids Storage \u00b6 `storage: ${ SID } :metadata.pending` = List < FileMetadata > // see below To optimise storage performance and reduce the need for database synchronization, metadata of newly created and modified files is not written to the storage database by the writing service directly. Instead, the relevant metadata is collected and appended to the :metadata.pending list. The corresponding storage service will then update its internal database with this information by continously watching this list and pulling new metadata. Sessions \u00b6 // ID = unique, external session identifier `session: ${ ID } :heartbeat.node` = string EX 60 s // RFC 3339 `session: ${ ID } :heartbeat.manager` = string EX 30 s // RFC 3339 `session: ${ ID } :slot` = string // slot ID `session: ${ ID } :orchestrator` = List < string > // orchestrator ID `session: ${ ID } :status` = Hashes { queuedAt = string // RFC 3339 pendingAt = string // RFC 3339 aliveAt = string // RFC 3339 terminatedAt = string // RFC 3339 } `session: ${ ID } :capabilities` = Hashes { requested = string // JSON actual = string // JSON } `session: ${ ID } :metadata` = Hashes { name = string build = string } `session: ${ ID } :storage` = string // storage ID `session: ${ ID } :telemetry.creation` = Hashes { traceID = string // root span / trace ID context = string // serialized span context } Orchestrators \u00b6 `orchestrator: ${ ID } ` = Hashes { type = 'local' | 'docker' | 'k8s' } `orchestrator: ${ ID } :heartbeat` = number EX 60 `orchestrator: ${ ID } :retain` = number EX 604800 // If this key is not set, the orchestrator metadata can be purged // Expires after 7 days and is refreshed by a live orchestrator. `orchestrator: ${ ID } :capabilities:platformName` = string `orchestrator: ${ ID } :capabilities:browsers` = Set < string > // explained below `orchestrator: ${ ID } :slots.reclaimed` = List < string > // slot ID `orchestrator: ${ ID } :slots.available` = List < string > // slot ID `orchestrator: ${ ID } :slots` = Set < string > // slot ID `orchestrator: ${ ID } :backlog` = List < string > // session ID `orchestrator: ${ ID } :pending` = List < string > // session ID *Browsers are represented by a string containing the browserName and browserVersion separated by :: . For example chrome::81.0.4044.113 or firefox::74.0.1 . Reliable queue documentation Metrics \u00b6 HTTP (at proxy) \u00b6 `metrics:http:requestsTotal: ${ method } ` = Hashes { < http - status - code > = number } `metrics:http:net.bytes.total` = Hashes { in = number out = number } Sessions \u00b6 `metrics:sessions:total` = Hashes { queued = number pending = number alive = number terminated = number } // TODO Figure out how to actually set this, maybe on state change based on the previous state? `metrics:sessions:duration.seconds.total` = Hashes { queued = number pending = number alive = number } `metrics:sessions:startup.histogram:count` `metrics:sessions:startup.histogram:sum` `metrics:sessions:startup.histogram:buckets` = Hashes { < bucket > = number } `metrics:sessions:log: ${ level } ` = Hashes { < session - log - code > = number } Orchestrator \u00b6 `metrics:slots:reclaimed.total` = Hashes { dead = number orphaned = number } Storage \u00b6 `metrics:storage:disk.bytes.total` = Hashes { < storage - id > = number } `metrics:storage:disk.bytes.used` = Hashes { < storage - id > = number } Garbage collection \u00b6 Special considerations have been taken to ensure that most keys expire on their own (e.g. active manager/storage/api metadata). Those that do not expire on their own (e.g. sessions) will be purged by a dedicated garbage collector service.","title":"Database"},{"location":"architecture/database/#database","text":"Outdated Due to a recent architectural overhaul, the information on this page is no longer applicable for versions beyond v0.5.1-beta .","title":"Database"},{"location":"architecture/database/#keys","text":"All metadata is stored in a key-value in-memory database called Redis . Below is a list of all keys that are currently in use, annotated with their type and format (if applicable).","title":"Keys"},{"location":"architecture/database/#root-lists","text":"`orchestrators` = Set < string > // uuids `sessions.active` = Set < string > // uuids `sessions.terminated` = Set < string > // uuids","title":"Root lists"},{"location":"architecture/database/#storage","text":"`storage: ${ SID } :metadata.pending` = List < FileMetadata > // see below To optimise storage performance and reduce the need for database synchronization, metadata of newly created and modified files is not written to the storage database by the writing service directly. Instead, the relevant metadata is collected and appended to the :metadata.pending list. The corresponding storage service will then update its internal database with this information by continously watching this list and pulling new metadata.","title":"Storage"},{"location":"architecture/database/#sessions","text":"// ID = unique, external session identifier `session: ${ ID } :heartbeat.node` = string EX 60 s // RFC 3339 `session: ${ ID } :heartbeat.manager` = string EX 30 s // RFC 3339 `session: ${ ID } :slot` = string // slot ID `session: ${ ID } :orchestrator` = List < string > // orchestrator ID `session: ${ ID } :status` = Hashes { queuedAt = string // RFC 3339 pendingAt = string // RFC 3339 aliveAt = string // RFC 3339 terminatedAt = string // RFC 3339 } `session: ${ ID } :capabilities` = Hashes { requested = string // JSON actual = string // JSON } `session: ${ ID } :metadata` = Hashes { name = string build = string } `session: ${ ID } :storage` = string // storage ID `session: ${ ID } :telemetry.creation` = Hashes { traceID = string // root span / trace ID context = string // serialized span context }","title":"Sessions"},{"location":"architecture/database/#orchestrators","text":"`orchestrator: ${ ID } ` = Hashes { type = 'local' | 'docker' | 'k8s' } `orchestrator: ${ ID } :heartbeat` = number EX 60 `orchestrator: ${ ID } :retain` = number EX 604800 // If this key is not set, the orchestrator metadata can be purged // Expires after 7 days and is refreshed by a live orchestrator. `orchestrator: ${ ID } :capabilities:platformName` = string `orchestrator: ${ ID } :capabilities:browsers` = Set < string > // explained below `orchestrator: ${ ID } :slots.reclaimed` = List < string > // slot ID `orchestrator: ${ ID } :slots.available` = List < string > // slot ID `orchestrator: ${ ID } :slots` = Set < string > // slot ID `orchestrator: ${ ID } :backlog` = List < string > // session ID `orchestrator: ${ ID } :pending` = List < string > // session ID *Browsers are represented by a string containing the browserName and browserVersion separated by :: . For example chrome::81.0.4044.113 or firefox::74.0.1 . Reliable queue documentation","title":"Orchestrators"},{"location":"architecture/database/#metrics","text":"","title":"Metrics"},{"location":"architecture/database/#http-at-proxy","text":"`metrics:http:requestsTotal: ${ method } ` = Hashes { < http - status - code > = number } `metrics:http:net.bytes.total` = Hashes { in = number out = number }","title":"HTTP (at proxy)"},{"location":"architecture/database/#sessions_1","text":"`metrics:sessions:total` = Hashes { queued = number pending = number alive = number terminated = number } // TODO Figure out how to actually set this, maybe on state change based on the previous state? `metrics:sessions:duration.seconds.total` = Hashes { queued = number pending = number alive = number } `metrics:sessions:startup.histogram:count` `metrics:sessions:startup.histogram:sum` `metrics:sessions:startup.histogram:buckets` = Hashes { < bucket > = number } `metrics:sessions:log: ${ level } ` = Hashes { < session - log - code > = number }","title":"Sessions"},{"location":"architecture/database/#orchestrator","text":"`metrics:slots:reclaimed.total` = Hashes { dead = number orphaned = number }","title":"Orchestrator"},{"location":"architecture/database/#storage_1","text":"`metrics:storage:disk.bytes.total` = Hashes { < storage - id > = number } `metrics:storage:disk.bytes.used` = Hashes { < storage - id > = number }","title":"Storage"},{"location":"architecture/database/#garbage-collection","text":"Special considerations have been taken to ensure that most keys expire on their own (e.g. active manager/storage/api metadata). Those that do not expire on their own (e.g. sessions) will be purged by a dedicated garbage collector service.","title":"Garbage collection"},{"location":"architecture/discovery/","text":"Service discovery \u00b6 This project uses multiple different services to accomplish a variety of tasks. Internal communication is handled through the Pub/Sub channel principle. However, larger payloads which are user-centric (i.e. caused by the user and returned to the user without modification) are transmitted over HTTP. This inherently requires a direct connection between the participating services. For this reason, a service discovery mechanism has been introduced. This mechanism generally operates in a semi-active manner but includes some additions to improve efficiency. The details are listed below Active discovery \u00b6 When a service wants to discover another service, it follows these steps: Build a ServiceDescriptor of the service to discover Derive a discovery messaging channel Send a request Listen on the general discovery response channel (non-specific to any service) Use any replies to establish a connection Note that requests are made to a specific channel where the channel address indicates which service to discovery. The reply, however, will be sent to a general channel. This allows for passive discovery, as explained below. Passive discovery \u00b6 Independent of the request-response mechanism, each service passively listens on the response channel. This allows capturing of responses to requests sent by other services. These responses will subsequently be stored in a local LRU cache. Especially for the proxy service, this increases performance (behind a load-balancer the load is randomly distributed to all proxies and if one user accesses some session it is likely that he will send further requests to that session in the future). Preemptive caching \u00b6 In addition to passive discovery, preemptive cache filling is done. On startup, a service broadcasts its endpoint to the discovery response channel. This fills the caches of other services thanks to passive discovery and reduces the number of round-trips required to zero (at the cost of some memory). Cache poisoning \u00b6 The previously mentioned cache can hold multiple endpoints for a given ServiceDescriptor . However, over time some of these endpoints may become unavailable. If such an endpoint is encountered, the corresponding cache entry will be purged and either one of the remaining cache entries will be tried or a new active discovery is started. This process is repeated for a limited number of times.","title":"Service discovery"},{"location":"architecture/discovery/#service-discovery","text":"This project uses multiple different services to accomplish a variety of tasks. Internal communication is handled through the Pub/Sub channel principle. However, larger payloads which are user-centric (i.e. caused by the user and returned to the user without modification) are transmitted over HTTP. This inherently requires a direct connection between the participating services. For this reason, a service discovery mechanism has been introduced. This mechanism generally operates in a semi-active manner but includes some additions to improve efficiency. The details are listed below","title":"Service discovery"},{"location":"architecture/discovery/#active-discovery","text":"When a service wants to discover another service, it follows these steps: Build a ServiceDescriptor of the service to discover Derive a discovery messaging channel Send a request Listen on the general discovery response channel (non-specific to any service) Use any replies to establish a connection Note that requests are made to a specific channel where the channel address indicates which service to discovery. The reply, however, will be sent to a general channel. This allows for passive discovery, as explained below.","title":"Active discovery"},{"location":"architecture/discovery/#passive-discovery","text":"Independent of the request-response mechanism, each service passively listens on the response channel. This allows capturing of responses to requests sent by other services. These responses will subsequently be stored in a local LRU cache. Especially for the proxy service, this increases performance (behind a load-balancer the load is randomly distributed to all proxies and if one user accesses some session it is likely that he will send further requests to that session in the future).","title":"Passive discovery"},{"location":"architecture/discovery/#preemptive-caching","text":"In addition to passive discovery, preemptive cache filling is done. On startup, a service broadcasts its endpoint to the discovery response channel. This fills the caches of other services thanks to passive discovery and reduces the number of round-trips required to zero (at the cost of some memory).","title":"Preemptive caching"},{"location":"architecture/discovery/#cache-poisoning","text":"The previously mentioned cache can hold multiple endpoints for a given ServiceDescriptor . However, over time some of these endpoints may become unavailable. If such an endpoint is encountered, the corresponding cache entry will be purged and either one of the remaining cache entries will be tried or a new active discovery is started. This process is repeated for a limited number of times.","title":"Cache poisoning"},{"location":"architecture/error-handling/","text":"Error handling \u00b6 In an ideal world, individual services are unable to fail by themselves. A service may reject a request as unfulfillable but always keeps processing. However due to the volatile nature of external resources like databases or cluster provisioners a monitoring system is required to handle outages. In this case a job system has been implemented where each job can have a dependency on an external resource. If this external resource dies, all dependent jobs will be cancelled. All root jobs of a component may then be restarted once the required resources become available again. Below is a rough outline of the architectural concepts involved. Parameter \u00b6 Parameters are constant values set at runtime, usually through the environment or command-line arguments. Resource \u00b6 Resources represent external data handlers like Redis, K8s or Storage. There are two types of resources, stateful and stateless. Stateful resources may report their current availability status while stateless ones are always reported as available. Resources have associated structures called Providers. A provider is responsible for creating handles to a resource. Each requested handle must have an associated ID. These IDs may not be unique if the underlying connection is shared. Handle IDs can be used for dependency tracking and job restarts in case a handle dies. Resources may have service-specific initialization jobs that are executed once they become available. These can be set on the corresponding resource provider upon creation. Contract \u00b6 Contracts atomically define behavior through a set of given input resource states and expected outputs. Contracts use placeholders for actual values much like class definitions. Request \u00b6 A specific instance of a contract with bound values is called a request. Job \u00b6 Jobs serve to fulfill one contract and may repeatedly do so. A job is considered healthy if it is able to fulfill its assigned contract. Task \u00b6 Jobs may have children called Tasks that process a single instance of a contract. For example the job could be an HTTP server which schedules new tasks for each incoming request. These children may then be terminated by the service if their resource handles go stale and the job may then take corresponding actions e.g. send an error code. Services \u00b6 Services are units that manage a set of related jobs and hold the required resource providers which can be configured through parameters. They are responsible for starting jobs and terminating those whose resources became stale. Jobs are restarted if the required resources are available again. Each job receives a handle to spawn ephemeral tasks which are not respawned. Additionally a future is passed that signals a clean shutdown condition \u2014 for proper SIGTERM handling.","title":"Error handling"},{"location":"architecture/error-handling/#error-handling","text":"In an ideal world, individual services are unable to fail by themselves. A service may reject a request as unfulfillable but always keeps processing. However due to the volatile nature of external resources like databases or cluster provisioners a monitoring system is required to handle outages. In this case a job system has been implemented where each job can have a dependency on an external resource. If this external resource dies, all dependent jobs will be cancelled. All root jobs of a component may then be restarted once the required resources become available again. Below is a rough outline of the architectural concepts involved.","title":"Error handling"},{"location":"architecture/error-handling/#parameter","text":"Parameters are constant values set at runtime, usually through the environment or command-line arguments.","title":"Parameter"},{"location":"architecture/error-handling/#resource","text":"Resources represent external data handlers like Redis, K8s or Storage. There are two types of resources, stateful and stateless. Stateful resources may report their current availability status while stateless ones are always reported as available. Resources have associated structures called Providers. A provider is responsible for creating handles to a resource. Each requested handle must have an associated ID. These IDs may not be unique if the underlying connection is shared. Handle IDs can be used for dependency tracking and job restarts in case a handle dies. Resources may have service-specific initialization jobs that are executed once they become available. These can be set on the corresponding resource provider upon creation.","title":"Resource"},{"location":"architecture/error-handling/#contract","text":"Contracts atomically define behavior through a set of given input resource states and expected outputs. Contracts use placeholders for actual values much like class definitions.","title":"Contract"},{"location":"architecture/error-handling/#request","text":"A specific instance of a contract with bound values is called a request.","title":"Request"},{"location":"architecture/error-handling/#job","text":"Jobs serve to fulfill one contract and may repeatedly do so. A job is considered healthy if it is able to fulfill its assigned contract.","title":"Job"},{"location":"architecture/error-handling/#task","text":"Jobs may have children called Tasks that process a single instance of a contract. For example the job could be an HTTP server which schedules new tasks for each incoming request. These children may then be terminated by the service if their resource handles go stale and the job may then take corresponding actions e.g. send an error code.","title":"Task"},{"location":"architecture/error-handling/#services","text":"Services are units that manage a set of related jobs and hold the required resource providers which can be configured through parameters. They are responsible for starting jobs and terminating those whose resources became stale. Jobs are restarted if the required resources are available again. Each job receives a handle to spawn ephemeral tasks which are not respawned. Additionally a future is passed that signals a clean shutdown condition \u2014 for proper SIGTERM handling.","title":"Services"},{"location":"architecture/services/","text":"Core services \u00b6 The WebGrid core consist of multiple services which interoperate to provide the reliable scaling functionality. Namely these are: Node Orchestrator (+ Provisioner) Manager Storage Proxy Below is a detailed description of the high-level responsibility of each service and their logical layout. The ones marked with a red asterisk can be scaled on-demand. Node \u00b6 At the core of each WebGrid are desktop environments which contain a Browser and additional software like the WebDriver[^e.g. geckodriver for Firefox] and a video encoder. The concept of a Node has been employed to abstract the underlying hardware & software logic of controlling the desktop environment, recording software and WebDriver away, leaving a clean interface for the Proxy[^Note that it rewrites incoming requests to change the external session ID to the one assigned by the local WebDriver]. Nodes start the driver, provide lifecycle information about the running session and handle cleanup when the session terminated. They have a very linear and simple service lifecycle: Start local driver Start screen recording Handle and forward WebDriver requests Terminate It does not matter whether or not a Node is running bare-bones on your Desktop or contained within larger infrastructure like a Kubernetes Cluster as long as the Proxy can reach it. Requests are forwarded to it through the proxy at * /session/${SID}/* Orchestrator \u00b6 Since the lifespan of a Node is tied to one specific session there needs to be an instance which handles the creation and deletion of them. The Orchestrator receives instructions from the Manager to create new Nodes and returns information on how to reach them. An Orchestrator can have multiple Node configurations available to choose from (e.g. different Browsers) and provides these capabilities to the Manager as a HashMap. Usually Orchestrators parse and forward the requests to create new nodes to a Provisioner like Docker or K8s however they can also just launch a local process. Slots \u00b6 One given Orchestrator may not have an unlimited amount of resources at its disposal. To constrain the number of Nodes that can exist at one given time, controlled by one given Orchestrator a concept of slots has been introduced. They are basically just IDs which are generated when the service is created and resemble \"Coupons\" which the Manager can use to request the creation of a Node. A slot is associated with one session on creation, if available and stays bound until the session is terminated at which point it is returned into the list of available slots for a new session to retrieve. Since any given service may die while processing requests each slot has a \"parent\" which is responsible for it. While a Node is running, it has the responsibility for its own slot. If it dies unexpectedly (and thus its heartbeat ceases to exist) the Orchestrator may reclaim its slot, adding it back to the list of available ones. This process is explained in more detail in the Workflows document. Here is an image of the slot lifecycle: Manager \u00b6 The Manager processes requests from clients to create new sessions by determining which orchestrators match the requested capabilities, requesting a slot and verifying the Node scheduling and startup process. A session leaves the responsibility of the manager as soon as the startup sequence has completed. For more details consult the Scheduling Workflow . It receives requests through the proxy at POST /session Storage \u00b6 In order to access files that sessions stored on disk a server instance with access to them is needed. For this purpose a storage service exists which delivers resources like screen recordings and manages storage occupancy by monitoring a local directory and potentially running cleanup tasks based on a local SQLite database. It receives requests through the proxy at GET /storage/${SID}/* . Proxy \u00b6 In order to route session traffic to the node that is hosting the session a reverse proxy is required. For this purpose (and load-balancing in a scenario with more than one manager instance) a service has been added. It listens on Redis key-space notifications to detect managers and nodes coming online or going offline, determined by their heartbeat. When a node comes online, the proxy routes all requests of the corresponding session ( /session/<ID>* ) to the node. For managers it routes requests matching POST /session to a randomly selected upstream from the list of alive managers. Garbage collector \u00b6 As mentioned in the database documentation , this service is responsible for removing old and unused keys from the database. Additionally, it has the task of resolving undefined states in the dataset which e.g. occured due to outages. This includes the termination of old sessions which have not been cleaned.","title":"Core services"},{"location":"architecture/services/#core-services","text":"The WebGrid core consist of multiple services which interoperate to provide the reliable scaling functionality. Namely these are: Node Orchestrator (+ Provisioner) Manager Storage Proxy Below is a detailed description of the high-level responsibility of each service and their logical layout. The ones marked with a red asterisk can be scaled on-demand.","title":"Core services"},{"location":"architecture/services/#node","text":"At the core of each WebGrid are desktop environments which contain a Browser and additional software like the WebDriver[^e.g. geckodriver for Firefox] and a video encoder. The concept of a Node has been employed to abstract the underlying hardware & software logic of controlling the desktop environment, recording software and WebDriver away, leaving a clean interface for the Proxy[^Note that it rewrites incoming requests to change the external session ID to the one assigned by the local WebDriver]. Nodes start the driver, provide lifecycle information about the running session and handle cleanup when the session terminated. They have a very linear and simple service lifecycle: Start local driver Start screen recording Handle and forward WebDriver requests Terminate It does not matter whether or not a Node is running bare-bones on your Desktop or contained within larger infrastructure like a Kubernetes Cluster as long as the Proxy can reach it. Requests are forwarded to it through the proxy at * /session/${SID}/*","title":"Node"},{"location":"architecture/services/#orchestrator","text":"Since the lifespan of a Node is tied to one specific session there needs to be an instance which handles the creation and deletion of them. The Orchestrator receives instructions from the Manager to create new Nodes and returns information on how to reach them. An Orchestrator can have multiple Node configurations available to choose from (e.g. different Browsers) and provides these capabilities to the Manager as a HashMap. Usually Orchestrators parse and forward the requests to create new nodes to a Provisioner like Docker or K8s however they can also just launch a local process.","title":"Orchestrator"},{"location":"architecture/services/#slots","text":"One given Orchestrator may not have an unlimited amount of resources at its disposal. To constrain the number of Nodes that can exist at one given time, controlled by one given Orchestrator a concept of slots has been introduced. They are basically just IDs which are generated when the service is created and resemble \"Coupons\" which the Manager can use to request the creation of a Node. A slot is associated with one session on creation, if available and stays bound until the session is terminated at which point it is returned into the list of available slots for a new session to retrieve. Since any given service may die while processing requests each slot has a \"parent\" which is responsible for it. While a Node is running, it has the responsibility for its own slot. If it dies unexpectedly (and thus its heartbeat ceases to exist) the Orchestrator may reclaim its slot, adding it back to the list of available ones. This process is explained in more detail in the Workflows document. Here is an image of the slot lifecycle:","title":"Slots"},{"location":"architecture/services/#manager","text":"The Manager processes requests from clients to create new sessions by determining which orchestrators match the requested capabilities, requesting a slot and verifying the Node scheduling and startup process. A session leaves the responsibility of the manager as soon as the startup sequence has completed. For more details consult the Scheduling Workflow . It receives requests through the proxy at POST /session","title":"Manager"},{"location":"architecture/services/#storage","text":"In order to access files that sessions stored on disk a server instance with access to them is needed. For this purpose a storage service exists which delivers resources like screen recordings and manages storage occupancy by monitoring a local directory and potentially running cleanup tasks based on a local SQLite database. It receives requests through the proxy at GET /storage/${SID}/* .","title":"Storage"},{"location":"architecture/services/#proxy","text":"In order to route session traffic to the node that is hosting the session a reverse proxy is required. For this purpose (and load-balancing in a scenario with more than one manager instance) a service has been added. It listens on Redis key-space notifications to detect managers and nodes coming online or going offline, determined by their heartbeat. When a node comes online, the proxy routes all requests of the corresponding session ( /session/<ID>* ) to the node. For managers it routes requests matching POST /session to a randomly selected upstream from the list of alive managers.","title":"Proxy"},{"location":"architecture/services/#garbage-collector","text":"As mentioned in the database documentation , this service is responsible for removing old and unused keys from the database. Additionally, it has the task of resolving undefined states in the dataset which e.g. occured due to outages. This includes the termination of old sessions which have not been cleaned.","title":"Garbage collector"},{"location":"architecture/structure/","text":"Project structure \u00b6 This page covers various topics that are required to understand the project structure both on a file-system as well as a conceptual level. Repository layout \u00b6 The repository contains multiple subfolders: /docs This documentation /core Core component /api API component /distribution Platform specific packaging scripts docker Docker images and stuff kubernetes Helm chart High level concepts \u00b6 The project has been divided into multiple hierarchical levels of abstraction. They are outlined in top-to-bottom order below. Components \u00b6 At the very top there are components which rest at the root directory of the project. These each provide a comprehensive feature-set of the grid and would be totally isolated in an ideal world. However, as some data sharing is required they access each other's data sources through clearly defined interfaces. Below are all components that are currently in existence and planned: Core \u00b6 The Core component houses all services that are critical to the operation of the grid and need to be redundant, resilient and highly performant. It is written in Rust to achieve these goals. API \u00b6 The API component provides an external interface to the internal metadata of the grid. As of now it is read-only and available through the Core components Proxy service. It is written in TypeScript and uses GraphQL as the query language. Dashboard \u00b6 A future component that is on the roadmap will be the Dashboard . It is expected to provide a comprehensive and user-friendly overview of the grid's status. Current plans are for it to be written using Svelte . Services \u00b6 Services are lower level puzzle pieces that each serve a distinct role and can be scaled individually. Currently, only the Core component makes use of this concept as every other component would only have one service. The Core services are described seperately . Jobs & Tasks \u00b6 In order to improve error handling and recovery a concept of Jobs & Tasks has been introduced. Each service process internally launches a set of jobs which each serve a single purpose e.g. serving HTTP POST requests from selenium clients in case of the manager. Each Job may spawn tasks which are ephemeral units that run once to e.g. serve an incoming request. To read more about jobs and how they improve error processing, head to the error handling page .","title":"Project structure"},{"location":"architecture/structure/#project-structure","text":"This page covers various topics that are required to understand the project structure both on a file-system as well as a conceptual level.","title":"Project structure"},{"location":"architecture/structure/#repository-layout","text":"The repository contains multiple subfolders: /docs This documentation /core Core component /api API component /distribution Platform specific packaging scripts docker Docker images and stuff kubernetes Helm chart","title":"Repository layout"},{"location":"architecture/structure/#high-level-concepts","text":"The project has been divided into multiple hierarchical levels of abstraction. They are outlined in top-to-bottom order below.","title":"High level concepts"},{"location":"architecture/structure/#components","text":"At the very top there are components which rest at the root directory of the project. These each provide a comprehensive feature-set of the grid and would be totally isolated in an ideal world. However, as some data sharing is required they access each other's data sources through clearly defined interfaces. Below are all components that are currently in existence and planned:","title":"Components"},{"location":"architecture/structure/#core","text":"The Core component houses all services that are critical to the operation of the grid and need to be redundant, resilient and highly performant. It is written in Rust to achieve these goals.","title":"Core"},{"location":"architecture/structure/#api","text":"The API component provides an external interface to the internal metadata of the grid. As of now it is read-only and available through the Core components Proxy service. It is written in TypeScript and uses GraphQL as the query language.","title":"API"},{"location":"architecture/structure/#dashboard","text":"A future component that is on the roadmap will be the Dashboard . It is expected to provide a comprehensive and user-friendly overview of the grid's status. Current plans are for it to be written using Svelte .","title":"Dashboard"},{"location":"architecture/structure/#services","text":"Services are lower level puzzle pieces that each serve a distinct role and can be scaled individually. Currently, only the Core component makes use of this concept as every other component would only have one service. The Core services are described seperately .","title":"Services"},{"location":"architecture/structure/#jobs-tasks","text":"In order to improve error handling and recovery a concept of Jobs & Tasks has been introduced. Each service process internally launches a set of jobs which each serve a single purpose e.g. serving HTTP POST requests from selenium clients in case of the manager. Each Job may spawn tasks which are ephemeral units that run once to e.g. serve an incoming request. To read more about jobs and how they improve error processing, head to the error handling page .","title":"Jobs &amp; Tasks"},{"location":"architecture/workflows/","text":"Workflows \u00b6 Outdated Due to a recent architectural overhaul, the information on this page is no longer applicable for versions beyond v0.5.1-beta . Below is a list of common process workflows that happen within WebGrid. They may help understanding how the grid operates internally. Scheduling workflow \u00b6 Orchestrator puts available slots into :slots.available on startup by running a reclaim cycle Manager receives client request, runs session creation workflow Manager pulls slot from orchestrators into session:<ID>:slot This operation has to be implemented using BLPOP and thus might crash between the BLPOP and SET operations, making the slot vanish. If this happens the orchestrators reclaim cycle may make this slot available again (more on that later) Manager pushes the sessionID into the orchestrators :backlog Orchestrator sequentially processes the backlog, moving messages into pending while provisioning the nodes Orchestrator notifies the manager Removes the sessionID from pending Sets the sessions :status:pendingAt Pushes its ID into the sessions :orchestrator key Manager watches the sessions :orchestrator key using BRPOPLPUSH for it to become available Manager runs health-check against node ( http://node/status ) Manager replies to client Manager sets :status:aliveAt property, effectively moving slot responsibility to the node from now on Node runs session termination workflow when client ends session If the orchestrator finds any sessions in its pending list on startup, it may fulfil these requests if the referenced session has not entered a terminated state (the manager ran into a timeout waiting for the orchestrator). Otherwise it may delete the task. Slot reclaim workflow \u00b6 Since any given component in the system may fail during slot interaction the orchestrator has to reclaim slots that have become stale/unused but are not properly returned. In order to do so it follows these steps, keeping an internal list of reclaimed slot-IDs: Iterate sessions that have not entered a terminated state and whose slot is owned by this orchestrator If :status:aliveAt is not set and there is no :heartbeat.manager , run session termination workflow Else if there is no :heartbeat.node , run session termination workflow Else the slot is still in use If not all slots are either reclaimed or in use, the missing slots may be added back to the list of available slots This reclaim cycle may be used as a cleanup cycle by the orchestrator to terminate any orphaned nodes that are not referenced by an alive session in the database. Changing the number of slots for an orchestrator \u00b6 This is done by the orchestrator on startup (target value is read from its configuration/environment). Adding new slots \u00b6 The number of slots may be increased by simply adding slots to orchestrator:<ID>:slots and orchestrator:<ID>:slots.available Removing active slots \u00b6 To decrease the slot amount, queue a number of BRPOP statements on the :slots.available and remove the returned slots from the :slots list Session creation workflow \u00b6 Generate ID Write initial information session:<ID>:status:queuedAt session:<ID>:capabilities:requested session:<ID>:downstream:* Append sessionID to sessions.active Session termination workflow \u00b6 Move sessions :slot back into orchestrators :slots.available Move sessions ID from sessions.active to sessions.terminated Set sessions :status:terminatedAt to the current time Delete :heartbeat.node","title":"Workflows"},{"location":"architecture/workflows/#workflows","text":"Outdated Due to a recent architectural overhaul, the information on this page is no longer applicable for versions beyond v0.5.1-beta . Below is a list of common process workflows that happen within WebGrid. They may help understanding how the grid operates internally.","title":"Workflows"},{"location":"architecture/workflows/#scheduling-workflow","text":"Orchestrator puts available slots into :slots.available on startup by running a reclaim cycle Manager receives client request, runs session creation workflow Manager pulls slot from orchestrators into session:<ID>:slot This operation has to be implemented using BLPOP and thus might crash between the BLPOP and SET operations, making the slot vanish. If this happens the orchestrators reclaim cycle may make this slot available again (more on that later) Manager pushes the sessionID into the orchestrators :backlog Orchestrator sequentially processes the backlog, moving messages into pending while provisioning the nodes Orchestrator notifies the manager Removes the sessionID from pending Sets the sessions :status:pendingAt Pushes its ID into the sessions :orchestrator key Manager watches the sessions :orchestrator key using BRPOPLPUSH for it to become available Manager runs health-check against node ( http://node/status ) Manager replies to client Manager sets :status:aliveAt property, effectively moving slot responsibility to the node from now on Node runs session termination workflow when client ends session If the orchestrator finds any sessions in its pending list on startup, it may fulfil these requests if the referenced session has not entered a terminated state (the manager ran into a timeout waiting for the orchestrator). Otherwise it may delete the task.","title":"Scheduling workflow"},{"location":"architecture/workflows/#slot-reclaim-workflow","text":"Since any given component in the system may fail during slot interaction the orchestrator has to reclaim slots that have become stale/unused but are not properly returned. In order to do so it follows these steps, keeping an internal list of reclaimed slot-IDs: Iterate sessions that have not entered a terminated state and whose slot is owned by this orchestrator If :status:aliveAt is not set and there is no :heartbeat.manager , run session termination workflow Else if there is no :heartbeat.node , run session termination workflow Else the slot is still in use If not all slots are either reclaimed or in use, the missing slots may be added back to the list of available slots This reclaim cycle may be used as a cleanup cycle by the orchestrator to terminate any orphaned nodes that are not referenced by an alive session in the database.","title":"Slot reclaim workflow"},{"location":"architecture/workflows/#changing-the-number-of-slots-for-an-orchestrator","text":"This is done by the orchestrator on startup (target value is read from its configuration/environment).","title":"Changing the number of slots for an orchestrator"},{"location":"architecture/workflows/#adding-new-slots","text":"The number of slots may be increased by simply adding slots to orchestrator:<ID>:slots and orchestrator:<ID>:slots.available","title":"Adding new slots"},{"location":"architecture/workflows/#removing-active-slots","text":"To decrease the slot amount, queue a number of BRPOP statements on the :slots.available and remove the returned slots from the :slots list","title":"Removing active slots"},{"location":"architecture/workflows/#session-creation-workflow","text":"Generate ID Write initial information session:<ID>:status:queuedAt session:<ID>:capabilities:requested session:<ID>:downstream:* Append sessionID to sessions.active","title":"Session creation workflow"},{"location":"architecture/workflows/#session-termination-workflow","text":"Move sessions :slot back into orchestrators :slots.available Move sessions ID from sessions.active to sessions.terminated Set sessions :status:terminatedAt to the current time Delete :heartbeat.node","title":"Session termination workflow"},{"location":"contribute/","text":"Introduction \u00b6 First off, thank you for considering contributing to WebGrid! You are awesome and we greatly appreciate any contributions \ud83d\ude42 Awesome Minions GIF from Awesome GIFs Following the guidelines outlined on the next few pages helps us focusing on maintaining and developing this project and allowing more time to assist you with your contributions. So please spent the time to read the sections that correspond to your area of contribution carefully! WebGrid is an open source project and we love to receive contributions from our community \u2014 you! There are many ways to contribute, from writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests or writing code which can be incorporated into WebGrid itself. If you are unsure how to contribute to a open source project in general, take a look at this resource or this one . If you find yourself wishing for a feature that doesn't exist in WebGrid, you are probably not alone. There are bound to be others out there with similar needs. Many of the features that WebGrid has today have been added because our users saw the need. Open an issue on our issues list on GitHub which describes the feature you would like to see, why you need it, and how it should work. Lastly, please be nice to everyone! You can read the code of conduct here , please adhere to it we really don't want to get out the ban-hammer.","title":"Introduction"},{"location":"contribute/#introduction","text":"First off, thank you for considering contributing to WebGrid! You are awesome and we greatly appreciate any contributions \ud83d\ude42 Awesome Minions GIF from Awesome GIFs Following the guidelines outlined on the next few pages helps us focusing on maintaining and developing this project and allowing more time to assist you with your contributions. So please spent the time to read the sections that correspond to your area of contribution carefully! WebGrid is an open source project and we love to receive contributions from our community \u2014 you! There are many ways to contribute, from writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests or writing code which can be incorporated into WebGrid itself. If you are unsure how to contribute to a open source project in general, take a look at this resource or this one . If you find yourself wishing for a feature that doesn't exist in WebGrid, you are probably not alone. There are bound to be others out there with similar needs. Many of the features that WebGrid has today have been added because our users saw the need. Open an issue on our issues list on GitHub which describes the feature you would like to see, why you need it, and how it should work. Lastly, please be nice to everyone! You can read the code of conduct here , please adhere to it we really don't want to get out the ban-hammer.","title":"Introduction"},{"location":"contribute/code-contrib/","text":"Code contributions \u00b6 We greatly appreciate any code contributions even if it is just a small typo fix in the documentation. You can take a look at the issue tracker for Available tasks which already have a solution outlined. If you are unsure on how to implement a change just ask, we are there to guide you through the process. You can also take a look at the list of Accepted tasks . These are accepted changes that do not have a sketch on how to solve them yet but if you are interested in approaching one we are going to assist you in solving it! Just comment on the issue of interest to let us know. Tip Did you know you can edit any documentation page directly just by clicking the edit button on the top right of each page? Please feel free to do so if you have found areas of improvement! Getting to know the project \u00b6 It can be a daunting task to get into a new project, we encountered it ourselves more than we'd like to admit. For this reason a comprehensive guide to the project structure, local developer setup and other topics is available in the Architecture tab . We strive to make the onboarding experience as simple and straightforward as possible so if you have any questions or ideas for improving it please open a ticket ! Conventions \u00b6 This project follows a few conventions regarding code contributions \u2014 below is a list of them. Commit messages \u00b6 Your commit messages should always be imparative, capitalized, without any punctuation at the end and able to complete the following sentence: If applied, this commit will <your-commit-message>. You can read more about how to write good commit messages and why its important over here . This blog post is used as a guideline for this repository! Gitmoji \u00b6 All commit messages should be prefixed with a GitHub Emoji that describes in one character what the commit is all about. For reference you should take a look at the gitmoji page ! PGP signing \u00b6 Every commit must be PGP signed. This can be achieved by either editing files directly on GitHub or setting up commit signing locally (make sure to upload your public key to GitHub if you sign locally). Workflow \u00b6 Below are steps which are usually followed for code contributions \u2014 use them to get acquainted to the process before contributing or if you are unsure on what follows next. 1. Solution sketch \u00b6 At the beginning of each code contribution a solution sketch has to exist. For some tasks like typo fixes this is a no-brainer but more complex tasks require some discussion up-front on how to approach a problem. This ensures that the result is compliant with project standards and doesn't create unexpected problems down the road! For issues from the Available category a solution sketch is already provided which makes it even easier to get started! 2. Implementing changes \u00b6 Once a solution sketch has been outlined you can start working on the code. Make sure to notify the maintainers so that the issue can be moved to the corresponding lifecycle stage preventing duplicate work. At this stage you setup your development environment (which may or may not be required depending on the changes as e.g. documentation changes can usually be done online using the GitHub Web Editor) and write code. If you have any questions ask on your issue ticket or submit a Draft Pull Request and comment on it if you need code-level assistance! 3. Code review \u00b6 If you are done with your changes or want to seek feedback from maintainers you push your changes and open a Pull Request. Make sure to add the original issue number to the description so it can be associated later. Take special considerations in naming your PR as it will be published in the Changelog \ud83d\ude09 The PR will be reviewed by other project members and automatic tests are run against your code. If everything is well, the changes will be approved and merged into the main branch. However, it is very common that changes are requested \u2014 don't feel bad about it, we provide you feedback to improve your already amazing contribution. That brings you back to the previous stage of Implementing changes . Once your changes have been approved, they are merged into the main branch and published during the next release cycle !","title":"Code contributions"},{"location":"contribute/code-contrib/#code-contributions","text":"We greatly appreciate any code contributions even if it is just a small typo fix in the documentation. You can take a look at the issue tracker for Available tasks which already have a solution outlined. If you are unsure on how to implement a change just ask, we are there to guide you through the process. You can also take a look at the list of Accepted tasks . These are accepted changes that do not have a sketch on how to solve them yet but if you are interested in approaching one we are going to assist you in solving it! Just comment on the issue of interest to let us know. Tip Did you know you can edit any documentation page directly just by clicking the edit button on the top right of each page? Please feel free to do so if you have found areas of improvement!","title":"Code contributions"},{"location":"contribute/code-contrib/#getting-to-know-the-project","text":"It can be a daunting task to get into a new project, we encountered it ourselves more than we'd like to admit. For this reason a comprehensive guide to the project structure, local developer setup and other topics is available in the Architecture tab . We strive to make the onboarding experience as simple and straightforward as possible so if you have any questions or ideas for improving it please open a ticket !","title":"Getting to know the project"},{"location":"contribute/code-contrib/#conventions","text":"This project follows a few conventions regarding code contributions \u2014 below is a list of them.","title":"Conventions"},{"location":"contribute/code-contrib/#commit-messages","text":"Your commit messages should always be imparative, capitalized, without any punctuation at the end and able to complete the following sentence: If applied, this commit will <your-commit-message>. You can read more about how to write good commit messages and why its important over here . This blog post is used as a guideline for this repository!","title":"Commit messages"},{"location":"contribute/code-contrib/#gitmoji","text":"All commit messages should be prefixed with a GitHub Emoji that describes in one character what the commit is all about. For reference you should take a look at the gitmoji page !","title":"Gitmoji"},{"location":"contribute/code-contrib/#pgp-signing","text":"Every commit must be PGP signed. This can be achieved by either editing files directly on GitHub or setting up commit signing locally (make sure to upload your public key to GitHub if you sign locally).","title":"PGP signing"},{"location":"contribute/code-contrib/#workflow","text":"Below are steps which are usually followed for code contributions \u2014 use them to get acquainted to the process before contributing or if you are unsure on what follows next.","title":"Workflow"},{"location":"contribute/code-contrib/#1-solution-sketch","text":"At the beginning of each code contribution a solution sketch has to exist. For some tasks like typo fixes this is a no-brainer but more complex tasks require some discussion up-front on how to approach a problem. This ensures that the result is compliant with project standards and doesn't create unexpected problems down the road! For issues from the Available category a solution sketch is already provided which makes it even easier to get started!","title":"1. Solution sketch"},{"location":"contribute/code-contrib/#2-implementing-changes","text":"Once a solution sketch has been outlined you can start working on the code. Make sure to notify the maintainers so that the issue can be moved to the corresponding lifecycle stage preventing duplicate work. At this stage you setup your development environment (which may or may not be required depending on the changes as e.g. documentation changes can usually be done online using the GitHub Web Editor) and write code. If you have any questions ask on your issue ticket or submit a Draft Pull Request and comment on it if you need code-level assistance!","title":"2. Implementing changes"},{"location":"contribute/code-contrib/#3-code-review","text":"If you are done with your changes or want to seek feedback from maintainers you push your changes and open a Pull Request. Make sure to add the original issue number to the description so it can be associated later. Take special considerations in naming your PR as it will be published in the Changelog \ud83d\ude09 The PR will be reviewed by other project members and automatic tests are run against your code. If everything is well, the changes will be approved and merged into the main branch. However, it is very common that changes are requested \u2014 don't feel bad about it, we provide you feedback to improve your already amazing contribution. That brings you back to the previous stage of Implementing changes . Once your changes have been approved, they are merged into the main branch and published during the next release cycle !","title":"3. Code review"},{"location":"contribute/code-of-conduct/","text":"Contributor Covenant Code of Conduct \u00b6 Our Pledge \u00b6 We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards \u00b6 Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities \u00b6 Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope \u00b6 This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement \u00b6 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at til@blechschmidt.dev . All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines \u00b6 Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction \u00b6 Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning \u00b6 Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban \u00b6 Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban \u00b6 Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution \u00b6 This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Code of conduct"},{"location":"contribute/code-of-conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"contribute/code-of-conduct/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"contribute/code-of-conduct/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"contribute/code-of-conduct/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"contribute/code-of-conduct/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"contribute/code-of-conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at til@blechschmidt.dev . All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"contribute/code-of-conduct/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"contribute/code-of-conduct/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"contribute/code-of-conduct/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"contribute/code-of-conduct/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"contribute/code-of-conduct/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"contribute/code-of-conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Attribution"},{"location":"contribute/dev-environment/","text":"Development environment \u00b6 This guide describes how to setup a local development environment. It covers the necessary build-tools and IDE setup. Prerequisites \u00b6 Docker & Kubernetes \u00b6 All components are distributed as Docker images and Helm charts. For this to work you have to install Docker , kubectl and Helm from their respective websites. Rust \u00b6 The core component uses Rust and some additional tools like linters and formatters. Visit rustup.rs and follow their install instructions Open a terminal in the core/ directory Run cargo check and cargo clippy to see if it works NodeJS \u00b6 The API component is written in TypeScript. To develop it you have to install NodeJS and yarn. Follow the install instructions on nodejs.org Open a terminal in the api/ directory Run yarn install and yarn build to see if it works Visual Studio Code \u00b6 While you can use any editor of your choice the recommended one is VSCode. This repository contains a workspace file with recommended extensions and settings. Download VSCode from their website Open the file webgrid.code-workspace Install all recommended installations (popup in the bottom right) Install rust-analyzer when prompted to do so Documentation \u00b6 If you want to build the documentation locally you have to install additional tools. Make sure you have a recent version of Python 3 installed before running the commands below. pip3 install 'mkdocs-git-revision-date-localized-plugin>=0.4' \\ 'mkdocs-material' \\ 'mkdocs-mermaid2-plugin' \\ 'mkdocs-codeinclude-plugin' \\ 'mkdocs-material-extensions' \\ 'mkdocs-simple-hooks' \\ 'git+http://github.com/TilBlechschmidt/mkdocs-helm' Bug The above command might not work on Windows. Write everything in one line and remove the \\ instead. Running locally \u00b6 Below are explanations on how to execute each component locally as well as some tips on how to reduce compile times during development. API \u00b6 To run the API, open a terminal in the api/ directory and execute the following commands: yarn codegen yarn start Code hot-reload It currently does not auto-reload if you change the sources. If you have some time to spare, why not contribute this feature? \ud83d\ude42 Core \u00b6 The core project contains a number of services. Each service can be started locally. For this to work you have to start a Redis database locally. To keep things simple we will just use Docker for this. docker run -it --rm --name webgrid-local-redis -p 6379 :6379 redis:alpine Once you have the database up and running you can start a specific component by opening a terminal in the core/ directory. Here is an example that shows all available components: cargo run -- -r redis://localhost/ --help To run a specific one, replace the --help flag with the name of a component listed in the help message. Note that every component has different requirements in terms of arguments, use the help flag on subcommands to find out more! Running in docker \u00b6 If you want to test the whole grid in Docker you can use docker-compose together with the Makefile. Building the images locally \u00b6 To build the images using the local code use one of the following commands: # Build all images make # Build the core image (excluding node images) make bundle-core # Build the api image make bundle-api # Build the node images make bundle-node # Clear all build caches make clean Running locally built images \u00b6 When not deployed by GitHub Actions the docker-compose file uses images tagged with webgrid/{node|core|api}:latest . These get created by the Makefile explained above. You can now start and stop the grid with the following commands: # Start grid make install # Stop grid (keeps the stored videos) make uninstall # Purge the associated volume and network docker network rm webgrid docker volume rm webgrid","title":"Development environment"},{"location":"contribute/dev-environment/#development-environment","text":"This guide describes how to setup a local development environment. It covers the necessary build-tools and IDE setup.","title":"Development environment"},{"location":"contribute/dev-environment/#prerequisites","text":"","title":"Prerequisites"},{"location":"contribute/dev-environment/#docker-kubernetes","text":"All components are distributed as Docker images and Helm charts. For this to work you have to install Docker , kubectl and Helm from their respective websites.","title":"Docker &amp; Kubernetes"},{"location":"contribute/dev-environment/#rust","text":"The core component uses Rust and some additional tools like linters and formatters. Visit rustup.rs and follow their install instructions Open a terminal in the core/ directory Run cargo check and cargo clippy to see if it works","title":"Rust"},{"location":"contribute/dev-environment/#nodejs","text":"The API component is written in TypeScript. To develop it you have to install NodeJS and yarn. Follow the install instructions on nodejs.org Open a terminal in the api/ directory Run yarn install and yarn build to see if it works","title":"NodeJS"},{"location":"contribute/dev-environment/#visual-studio-code","text":"While you can use any editor of your choice the recommended one is VSCode. This repository contains a workspace file with recommended extensions and settings. Download VSCode from their website Open the file webgrid.code-workspace Install all recommended installations (popup in the bottom right) Install rust-analyzer when prompted to do so","title":"Visual Studio Code"},{"location":"contribute/dev-environment/#documentation","text":"If you want to build the documentation locally you have to install additional tools. Make sure you have a recent version of Python 3 installed before running the commands below. pip3 install 'mkdocs-git-revision-date-localized-plugin>=0.4' \\ 'mkdocs-material' \\ 'mkdocs-mermaid2-plugin' \\ 'mkdocs-codeinclude-plugin' \\ 'mkdocs-material-extensions' \\ 'mkdocs-simple-hooks' \\ 'git+http://github.com/TilBlechschmidt/mkdocs-helm' Bug The above command might not work on Windows. Write everything in one line and remove the \\ instead.","title":"Documentation"},{"location":"contribute/dev-environment/#running-locally","text":"Below are explanations on how to execute each component locally as well as some tips on how to reduce compile times during development.","title":"Running locally"},{"location":"contribute/dev-environment/#api","text":"To run the API, open a terminal in the api/ directory and execute the following commands: yarn codegen yarn start Code hot-reload It currently does not auto-reload if you change the sources. If you have some time to spare, why not contribute this feature? \ud83d\ude42","title":"API"},{"location":"contribute/dev-environment/#core","text":"The core project contains a number of services. Each service can be started locally. For this to work you have to start a Redis database locally. To keep things simple we will just use Docker for this. docker run -it --rm --name webgrid-local-redis -p 6379 :6379 redis:alpine Once you have the database up and running you can start a specific component by opening a terminal in the core/ directory. Here is an example that shows all available components: cargo run -- -r redis://localhost/ --help To run a specific one, replace the --help flag with the name of a component listed in the help message. Note that every component has different requirements in terms of arguments, use the help flag on subcommands to find out more!","title":"Core"},{"location":"contribute/dev-environment/#running-in-docker","text":"If you want to test the whole grid in Docker you can use docker-compose together with the Makefile.","title":"Running in docker"},{"location":"contribute/dev-environment/#building-the-images-locally","text":"To build the images using the local code use one of the following commands: # Build all images make # Build the core image (excluding node images) make bundle-core # Build the api image make bundle-api # Build the node images make bundle-node # Clear all build caches make clean","title":"Building the images locally"},{"location":"contribute/dev-environment/#running-locally-built-images","text":"When not deployed by GitHub Actions the docker-compose file uses images tagged with webgrid/{node|core|api}:latest . These get created by the Makefile explained above. You can now start and stop the grid with the following commands: # Start grid make install # Stop grid (keeps the stored videos) make uninstall # Purge the associated volume and network docker network rm webgrid docker volume rm webgrid","title":"Running locally built images"},{"location":"contribute/issues/","text":"Issue reporting \u00b6 Creating an issue is the easiest way to contribute to the project! All issues are tracked using the GitHub Issue Board . There are four different types of issues: Type Description Issues that affect the functionality of the software Feature requests or ideas that would improve the project Improvements to code quality, readability, documentation or architecture General questions about the project, how it works and for non-documented features Reporting \u00b6 If you have a question, encountered a bug or want to suggest a feature please don't hesitate to open a ticket \u2014 Community feedback helps the project grow and adapt in the best possible way! Preventing duplicates Make sure to browse the issue backlog and previously closed issues as well as the FAQ to see if somebody else already covered your topic! This helps us focus on implementing your ideas and helping you with issues instead of housekeeping duplicates. Lifecycle \u00b6 Each ticket goes through the following lifecycle outlined below: stateDiagram [*] --> Pending Pending --> Available Pending --> Accepted Accepted --> Available Available --> InProgress InProgress --> Completed InProgress --> Abandoned Pending --> Abandoned Accepted --> Abandoned Pending --> Duplicate Completed --> [*] Abandoned --> [*] Duplicate --> [*] In addition to the states shown above, it may enter one of the following states from almost any other state: Status Description Waiting for another issue to be resolved Waiting for external response/feedback to continue down the issue lifecycle The proposed solution needs to be revised and changed according to maintainer feedback A solution has been proposed and needs to be reviewed by a maintainer For a more detailed description of all possible states, consult the repositories label page . Pull requests \u00b6 In addition to the labels of issues, Pull Request may have additional labels which identify the type of version bump required. Change Description New feature that breaks backwards compatibility and requires a manual upgrade Backwards compatible feature addition that can be upgraded automatically Minor changes like backwards compatible bug fixes and documentation changes","title":"Issue reporting"},{"location":"contribute/issues/#issue-reporting","text":"Creating an issue is the easiest way to contribute to the project! All issues are tracked using the GitHub Issue Board . There are four different types of issues: Type Description Issues that affect the functionality of the software Feature requests or ideas that would improve the project Improvements to code quality, readability, documentation or architecture General questions about the project, how it works and for non-documented features","title":"Issue reporting"},{"location":"contribute/issues/#reporting","text":"If you have a question, encountered a bug or want to suggest a feature please don't hesitate to open a ticket \u2014 Community feedback helps the project grow and adapt in the best possible way! Preventing duplicates Make sure to browse the issue backlog and previously closed issues as well as the FAQ to see if somebody else already covered your topic! This helps us focus on implementing your ideas and helping you with issues instead of housekeeping duplicates.","title":"Reporting"},{"location":"contribute/issues/#lifecycle","text":"Each ticket goes through the following lifecycle outlined below: stateDiagram [*] --> Pending Pending --> Available Pending --> Accepted Accepted --> Available Available --> InProgress InProgress --> Completed InProgress --> Abandoned Pending --> Abandoned Accepted --> Abandoned Pending --> Duplicate Completed --> [*] Abandoned --> [*] Duplicate --> [*] In addition to the states shown above, it may enter one of the following states from almost any other state: Status Description Waiting for another issue to be resolved Waiting for external response/feedback to continue down the issue lifecycle The proposed solution needs to be revised and changed according to maintainer feedback A solution has been proposed and needs to be reviewed by a maintainer For a more detailed description of all possible states, consult the repositories label page .","title":"Lifecycle"},{"location":"contribute/issues/#pull-requests","text":"In addition to the labels of issues, Pull Request may have additional labels which identify the type of version bump required. Change Description New feature that breaks backwards compatibility and requires a manual upgrade Backwards compatible feature addition that can be upgraded automatically Minor changes like backwards compatible bug fixes and documentation changes","title":"Pull requests"},{"location":"contribute/release/","text":"Release workflow \u00b6 The project generally follows the SemVer 2.0 convention. A new release usually follows these steps: Contributors open PRs Changes get merged into main branch GitHub Actions creates a draft release and updates it after every contribution Once a large enough number of contributions have accumulated the draft is published GitHub Actions release pipeline takes the following steps Build all components Create and push Docker images Publish new documentation and Helm chart Attach executables to GitHub Release","title":"Release workflow"},{"location":"contribute/release/#release-workflow","text":"The project generally follows the SemVer 2.0 convention. A new release usually follows these steps: Contributors open PRs Changes get merged into main branch GitHub Actions creates a draft release and updates it after every contribution Once a large enough number of contributions have accumulated the draft is published GitHub Actions release pipeline takes the following steps Build all components Create and push Docker images Publish new documentation and Helm chart Attach executables to GitHub Release","title":"Release workflow"},{"location":"features/api/","text":"API \u00b6 The grid exposes an API which provides read-only access to the status and metadata of sessions and some other internal components which might be of interest. It uses the GraphQL query language to provide predictable and typed responses and is available under the following URL: http://<your-webgrid-address>/api Tip When opening the API in a browser, a GraphQL Playground opens up which provides syntax highlighting, code completion and a complete documentation! Session metadata \u00b6 If you have attached metadata to your session as described over here , you can search for your session by using regular expressions. To do so, you can use a query that looks like this: query { session { query(fields: [ { key: \"project\", regex: \"^tardis$\" }, { key: \"answer\", regex: \"\\\\d+\" }, ]) { id } } } You can also fetch the latest sessions or retrieve details of a session given its identifier. For more details, consult the self-documenting API at /api .","title":"API"},{"location":"features/api/#api","text":"The grid exposes an API which provides read-only access to the status and metadata of sessions and some other internal components which might be of interest. It uses the GraphQL query language to provide predictable and typed responses and is available under the following URL: http://<your-webgrid-address>/api Tip When opening the API in a browser, a GraphQL Playground opens up which provides syntax highlighting, code completion and a complete documentation!","title":"API"},{"location":"features/api/#session-metadata","text":"If you have attached metadata to your session as described over here , you can search for your session by using regular expressions. To do so, you can use a query that looks like this: query { session { query(fields: [ { key: \"project\", regex: \"^tardis$\" }, { key: \"answer\", regex: \"\\\\d+\" }, ]) { id } } } You can also fetch the latest sessions or retrieve details of a session given its identifier. For more details, consult the self-documenting API at /api .","title":"Session metadata"},{"location":"features/capabilities/","text":"Extensions & Metadata \u00b6 While WebGrid supports all standard capabilities, there is a number of extensions. These allow you to control features like screen recording and idle timeouts. Additionally, they can be used to attach metadata to a session. Later on, this metadata can be used to search for your session! To get started, you have to create a map which contains all WebGrid specific settings. This map will later on be stored in the webgrid:options key of the capability object. The way to do this differs for each selenium library but below are a few examples to get you started. Java DesiredCapabilities desiredCapabilities = new DesiredCapabilities (); final Map < String , Object > webgridOptions = new HashMap <> (); // Put anything in your options map! See below for a list of whats available. desiredCapabilities . setCapability ( \"webgrid:options\" , webgridOptions ); Rust let mut caps = DesiredCapabilities :: firefox (); // Put anything in your options map! See below for a list of whats available. caps . add_subkey ( \"webgrid:options\" , /* Option key */ , /* Option value */ ) ? ; WebDriver :: new ( endpoint , & caps ). await ? Disabling screen recording \u00b6 We have optimized the heck out of screen recordings! They use almost no bandwidth and minimal CPU. For this reason, they are enabled globally if you have configured a storage backend. However, should you for some reason decide that you do not want to record a session, you can set the disableRecording flag in the webgrid:options capabilities. Java webgridOptions . put ( \"disableRecording\" , true ); Rust caps . add_subkey ( \"webgrid:options\" , \"disableRecording\" , true ); Globally disable recordings If you do not want recordings for any sessions, just do not configure a storage backend. Refer the corresponding installation guide on how to not do so! Overwriting idle timeout \u00b6 To conserve resources, each session terminates automatically when it does not receive a command from a client within a certain time period. This is especially useful in scenarios where the client may have crashed. Since the protocol is not connection oriented, there is no other way to detect such a situation. The default timeout is set to about 10 minutes. When setting up the grid, you have to opportunity to set a different global default. However, maybe just some of your clients need to stay idle for a long time while others do not. For such situations, you can overwrite the idle timeout on a per-session basis by setting the idleTimeout key in the webgrid:options capabilities to any numeric value in seconds. Java webgridOptions . put ( \"idleTimeout\" , 3600 ); Rust caps . add_subkey ( \"webgrid:options\" , \"idleTimeout\" , 3600 ); Very long timeouts Setting a very long timeout may cause issues. As it is virtually impossible for the grid to detect a client that has crashed or otherwise disconnected in a non-clean fashion, such a sessions may become \"orphaned\" and stick around blocking resources for the timeout you set. Attaching metadata \u00b6 When you run hundreds of sessions on the grid and do not have a way to store session identifiers, it can become hard to identify that one session after the fact \u2014 or maybe you want to run statistics on how many sessions each project has created in the last week. To solve this, you can attach arbitrary key-value metadata to each session by passing a map to the metadata key in the webgrid:options capabilities. Java final Map < String , String > metadata = new HashMap <> (); metadata . put ( \"project\" , \"tardis\" ); metadata . put ( \"pipeline\" , \"#42\" ); webgridOptions . put ( \"metadata\" , metadata ); Rust let mut metadata = HashMap :: new (); metadata . insert ( \"project\" , \"tardis\" ); metadata . insert ( \"pipeline\" , \"#42\" ); caps . add_subkey ( \"webgrid:options\" , \"metadata\" , metadata ); Modifying metadata at runtime \u00b6 Setting metadata up-front is nice. However, in certain scenarios you might want to attach additional or modify existing metadata while the session is running. For this, an extension command is available at /session/<id>/webgrid/metadata . You can either make a POST request to this URL with a JSON object as the request body, or use your libraries support for extension commands if available. In the future, support for legacy libraries which do not support extension commands will be added (using cookies with special names). cURL This simple example uses the popular cURL command line utility. While it is recommended to use extension commands with your driver, this is technically possible \ud83d\ude09 curl --request POST \\ --header \"Content-Type: application/json\" \\ --data '{ \"status\": \"success\" }' http://<your-grid>/session/<your-session-id>/webgrid/metadata Rust Below is an example implementation of a metadata modification extension command for the thirtyfour Rust library. struct WebgridMetadataCommand { fields : HashMap < String , String > , } impl WebgridMetadataCommand { pub fn new () -> Self { Self { fields : HashMap :: new (), } } pub fn with_field ( key : String , value : String ) -> Self { let mut instance = Self :: new (); instance . add ( key , value ); instance } pub fn add ( & mut self , key : String , value : String ) { self . fields . insert ( key , value ); } } impl ExtensionCommand for WebgridMetadataCommand { fn parameters_json ( & self ) -> Option < serde_json :: Value > { serde_json :: to_value ( self . fields . clone ()). ok () } fn method ( & self ) -> thirtyfour :: RequestMethod { thirtyfour :: RequestMethod :: Post } fn endpoint ( & self ) -> String { \"/webgrid/metadata\" . into () } } // Usage let metadata_command = WebgridMetadataCommand :: with_field ( \"answer\" . into (), \"42\" . into ()); driver . extension_command ( metadata_command ). await . ok ();","title":"Extensions & Metadata"},{"location":"features/capabilities/#extensions-metadata","text":"While WebGrid supports all standard capabilities, there is a number of extensions. These allow you to control features like screen recording and idle timeouts. Additionally, they can be used to attach metadata to a session. Later on, this metadata can be used to search for your session! To get started, you have to create a map which contains all WebGrid specific settings. This map will later on be stored in the webgrid:options key of the capability object. The way to do this differs for each selenium library but below are a few examples to get you started. Java DesiredCapabilities desiredCapabilities = new DesiredCapabilities (); final Map < String , Object > webgridOptions = new HashMap <> (); // Put anything in your options map! See below for a list of whats available. desiredCapabilities . setCapability ( \"webgrid:options\" , webgridOptions ); Rust let mut caps = DesiredCapabilities :: firefox (); // Put anything in your options map! See below for a list of whats available. caps . add_subkey ( \"webgrid:options\" , /* Option key */ , /* Option value */ ) ? ; WebDriver :: new ( endpoint , & caps ). await ?","title":"Extensions &amp; Metadata"},{"location":"features/capabilities/#disabling-screen-recording","text":"We have optimized the heck out of screen recordings! They use almost no bandwidth and minimal CPU. For this reason, they are enabled globally if you have configured a storage backend. However, should you for some reason decide that you do not want to record a session, you can set the disableRecording flag in the webgrid:options capabilities. Java webgridOptions . put ( \"disableRecording\" , true ); Rust caps . add_subkey ( \"webgrid:options\" , \"disableRecording\" , true ); Globally disable recordings If you do not want recordings for any sessions, just do not configure a storage backend. Refer the corresponding installation guide on how to not do so!","title":"Disabling screen recording"},{"location":"features/capabilities/#overwriting-idle-timeout","text":"To conserve resources, each session terminates automatically when it does not receive a command from a client within a certain time period. This is especially useful in scenarios where the client may have crashed. Since the protocol is not connection oriented, there is no other way to detect such a situation. The default timeout is set to about 10 minutes. When setting up the grid, you have to opportunity to set a different global default. However, maybe just some of your clients need to stay idle for a long time while others do not. For such situations, you can overwrite the idle timeout on a per-session basis by setting the idleTimeout key in the webgrid:options capabilities to any numeric value in seconds. Java webgridOptions . put ( \"idleTimeout\" , 3600 ); Rust caps . add_subkey ( \"webgrid:options\" , \"idleTimeout\" , 3600 ); Very long timeouts Setting a very long timeout may cause issues. As it is virtually impossible for the grid to detect a client that has crashed or otherwise disconnected in a non-clean fashion, such a sessions may become \"orphaned\" and stick around blocking resources for the timeout you set.","title":"Overwriting idle timeout"},{"location":"features/capabilities/#attaching-metadata","text":"When you run hundreds of sessions on the grid and do not have a way to store session identifiers, it can become hard to identify that one session after the fact \u2014 or maybe you want to run statistics on how many sessions each project has created in the last week. To solve this, you can attach arbitrary key-value metadata to each session by passing a map to the metadata key in the webgrid:options capabilities. Java final Map < String , String > metadata = new HashMap <> (); metadata . put ( \"project\" , \"tardis\" ); metadata . put ( \"pipeline\" , \"#42\" ); webgridOptions . put ( \"metadata\" , metadata ); Rust let mut metadata = HashMap :: new (); metadata . insert ( \"project\" , \"tardis\" ); metadata . insert ( \"pipeline\" , \"#42\" ); caps . add_subkey ( \"webgrid:options\" , \"metadata\" , metadata );","title":"Attaching metadata"},{"location":"features/capabilities/#modifying-metadata-at-runtime","text":"Setting metadata up-front is nice. However, in certain scenarios you might want to attach additional or modify existing metadata while the session is running. For this, an extension command is available at /session/<id>/webgrid/metadata . You can either make a POST request to this URL with a JSON object as the request body, or use your libraries support for extension commands if available. In the future, support for legacy libraries which do not support extension commands will be added (using cookies with special names). cURL This simple example uses the popular cURL command line utility. While it is recommended to use extension commands with your driver, this is technically possible \ud83d\ude09 curl --request POST \\ --header \"Content-Type: application/json\" \\ --data '{ \"status\": \"success\" }' http://<your-grid>/session/<your-session-id>/webgrid/metadata Rust Below is an example implementation of a metadata modification extension command for the thirtyfour Rust library. struct WebgridMetadataCommand { fields : HashMap < String , String > , } impl WebgridMetadataCommand { pub fn new () -> Self { Self { fields : HashMap :: new (), } } pub fn with_field ( key : String , value : String ) -> Self { let mut instance = Self :: new (); instance . add ( key , value ); instance } pub fn add ( & mut self , key : String , value : String ) { self . fields . insert ( key , value ); } } impl ExtensionCommand for WebgridMetadataCommand { fn parameters_json ( & self ) -> Option < serde_json :: Value > { serde_json :: to_value ( self . fields . clone ()). ok () } fn method ( & self ) -> thirtyfour :: RequestMethod { thirtyfour :: RequestMethod :: Post } fn endpoint ( & self ) -> String { \"/webgrid/metadata\" . into () } } // Usage let metadata_command = WebgridMetadataCommand :: with_field ( \"answer\" . into (), \"42\" . into ()); driver . extension_command ( metadata_command ). await . ok ();","title":"Modifying metadata at runtime"},{"location":"features/hybrid-grid/","text":"Hybrid grid \u00b6 The Getting Started Guide covers basic setups in either Kubernetes or Docker. However, in certain scenarios it might be required to include other devices that can't be enslaved to a cluster. Imagine a software testing use-case where you have the following setup: Chrome & Firefox pods running in K8s Safari on external Mac Mini Edge on external Windows PC All these devices can be collated behind a single grid endpoint. In theory, every component of the grid can be hosted in and scaled to any device regardless of whether or not it is in the cluster or not. However, for larger instances it is recommended to run the central grid components in Kubernetes and add external devices to extend its capabilities with e.g. Safari Browsers. Requirements \u00b6 In order to add external devices a few requirements have to be met: Proxys and Manager pods have to be able to reach the device Redis database has to be accessible by the device Once these prerequisites are met, continue to the next sections. Local orchestrator \u00b6 The orchestrator service is responsible for scheduling resources like Kubernetes pods or Docker containers which then in turn run the browsers. To use a local browser like Safari a single-instance orchestrator is required. Todo This feature is on the horizon. Even though the implementation has not yet been started, it has been incorporated during the design stage and its complexity is rather mild. If you need this feature, please open up an Issue or +1 an existing one regarding this feature.","title":"Hybrid grid"},{"location":"features/hybrid-grid/#hybrid-grid","text":"The Getting Started Guide covers basic setups in either Kubernetes or Docker. However, in certain scenarios it might be required to include other devices that can't be enslaved to a cluster. Imagine a software testing use-case where you have the following setup: Chrome & Firefox pods running in K8s Safari on external Mac Mini Edge on external Windows PC All these devices can be collated behind a single grid endpoint. In theory, every component of the grid can be hosted in and scaled to any device regardless of whether or not it is in the cluster or not. However, for larger instances it is recommended to run the central grid components in Kubernetes and add external devices to extend its capabilities with e.g. Safari Browsers.","title":"Hybrid grid"},{"location":"features/hybrid-grid/#requirements","text":"In order to add external devices a few requirements have to be met: Proxys and Manager pods have to be able to reach the device Redis database has to be accessible by the device Once these prerequisites are met, continue to the next sections.","title":"Requirements"},{"location":"features/hybrid-grid/#local-orchestrator","text":"The orchestrator service is responsible for scheduling resources like Kubernetes pods or Docker containers which then in turn run the browsers. To use a local browser like Safari a single-instance orchestrator is required. Todo This feature is on the horizon. Even though the implementation has not yet been started, it has been incorporated during the design stage and its complexity is rather mild. If you need this feature, please open up an Issue or +1 an existing one regarding this feature.","title":"Local orchestrator"},{"location":"features/screen-recording/","text":"Screen recording \u00b6 The grid is capable of capturing the screen for each browser session you create. This feature is enabled by default in a local Docker instance, but requires some additional configuration in a cluster. Warning By default, this feature is disabled in the Kubernetes helm chart as it requires additional setup. Details on how to get up and running can be found on the Kubernetes storage page ! Session ID \u00b6 In order to view or embed a video you need to retrieve its unique session identifier. The simplest method is through your client library \u2014 below are a few examples: Java FirefoxOptions firefoxOptions = new FirefoxOptions (); WebDriver driver = new RemoteWebDriver ( new URL ( \"http://localhost:8080\" ), firefoxOptions ); driver . get ( \"http://dcuk.com\" ); SessionId session = (( RemoteWebDriver ) driver ). getSessionId (); System . out . println ( \"Session id: \" + session . toString ()); System . in . read (); driver . quit (); Python from selenium import webdriver firefox_options = webdriver . FirefoxOptions () driver = webdriver . Remote ( command_executor = 'http://localhost:8080' , options = firefox_options ) driver . get ( \"http://www.google.com\" ) print ( \"Session id: \" + driver . session_id ); input ( \"Press Enter to quit...\" ) driver . quit () Retrieving session ID from API Alternatively, you can get a list of all (or just active) sessions from the grid API. Note however that the list can be very large, so it might be hard to identify yours. Use the following query to get a list of sessions e.g. by pasting it into the API explorer or using a GraphQL client: query { session { latest { id metadata { client { key value } } } } } It returns you a list of all sessions with their corresponding identifiers and user-assigned metadata. To identify your session uniquely using metadata, head over to the API documentation . Embedding \u00b6 You can embed the browser recording directly into your existing tools by using the JavaScript SDK. You need to import the JavaScript module and bind it to a DOM element of your choice. All CSS used is scoped to the webgrid class so it should not affect your website as long as you don't use this class in your styles. < body > < div id = \"<your-identifier>\" ></ div > < script type = \"module\" > import { WebGridVideo } from 'http://<your-webgrid-address>/embed' ; new WebGridVideo ({ target : document . getElementById ( \"<your-identifier>\" ), props : { sessionID : '<your-session-id>' , } }); </ script > </ body > By default, the script tries to guess the webgrid address from the import URL. This behaviour can be overruled by passing the host: '<your-webgrid-address>' property in the props object. Especially when fetching the script from within a static page builder which embeds it directly, this is required as the host is evaluated at runtime not at request time. Viewing \u00b6 If you want to monitor your session manually you may use the dashboard provided by the grid. To do so just visit it at http://<your-webgrid-address> (without any path) and enter the previously obtained session ID. Danger This dashboard is not supposed to be embedded into other pages through iframes or other means. For embedding refer the embedding section!","title":"Screen recording"},{"location":"features/screen-recording/#screen-recording","text":"The grid is capable of capturing the screen for each browser session you create. This feature is enabled by default in a local Docker instance, but requires some additional configuration in a cluster. Warning By default, this feature is disabled in the Kubernetes helm chart as it requires additional setup. Details on how to get up and running can be found on the Kubernetes storage page !","title":"Screen recording"},{"location":"features/screen-recording/#session-id","text":"In order to view or embed a video you need to retrieve its unique session identifier. The simplest method is through your client library \u2014 below are a few examples: Java FirefoxOptions firefoxOptions = new FirefoxOptions (); WebDriver driver = new RemoteWebDriver ( new URL ( \"http://localhost:8080\" ), firefoxOptions ); driver . get ( \"http://dcuk.com\" ); SessionId session = (( RemoteWebDriver ) driver ). getSessionId (); System . out . println ( \"Session id: \" + session . toString ()); System . in . read (); driver . quit (); Python from selenium import webdriver firefox_options = webdriver . FirefoxOptions () driver = webdriver . Remote ( command_executor = 'http://localhost:8080' , options = firefox_options ) driver . get ( \"http://www.google.com\" ) print ( \"Session id: \" + driver . session_id ); input ( \"Press Enter to quit...\" ) driver . quit () Retrieving session ID from API Alternatively, you can get a list of all (or just active) sessions from the grid API. Note however that the list can be very large, so it might be hard to identify yours. Use the following query to get a list of sessions e.g. by pasting it into the API explorer or using a GraphQL client: query { session { latest { id metadata { client { key value } } } } } It returns you a list of all sessions with their corresponding identifiers and user-assigned metadata. To identify your session uniquely using metadata, head over to the API documentation .","title":"Session ID"},{"location":"features/screen-recording/#embedding","text":"You can embed the browser recording directly into your existing tools by using the JavaScript SDK. You need to import the JavaScript module and bind it to a DOM element of your choice. All CSS used is scoped to the webgrid class so it should not affect your website as long as you don't use this class in your styles. < body > < div id = \"<your-identifier>\" ></ div > < script type = \"module\" > import { WebGridVideo } from 'http://<your-webgrid-address>/embed' ; new WebGridVideo ({ target : document . getElementById ( \"<your-identifier>\" ), props : { sessionID : '<your-session-id>' , } }); </ script > </ body > By default, the script tries to guess the webgrid address from the import URL. This behaviour can be overruled by passing the host: '<your-webgrid-address>' property in the props object. Especially when fetching the script from within a static page builder which embeds it directly, this is required as the host is evaluated at runtime not at request time.","title":"Embedding"},{"location":"features/screen-recording/#viewing","text":"If you want to monitor your session manually you may use the dashboard provided by the grid. To do so just visit it at http://<your-webgrid-address> (without any path) and enter the previously obtained session ID. Danger This dashboard is not supposed to be embedded into other pages through iframes or other means. For embedding refer the embedding section!","title":"Viewing"},{"location":"guides/k3s/","text":"Deploying in K3s \u00b6 This guide will explain how to get a basic but feature-complete instance of WebGrid up and running in a cluster using k3s by Rancher . It will assume that you have a basic k3s cluster running and a working kubectl and helm locally available. Installing with Helm \u00b6 To get started quickly, we will be deploying the demo chart. It contains the core grid along with external services that are required to get additional features like video recordings running. # Add the repository helm repo add webgrid https://webgrid.dev/ # List all available versions helm search repo --versions --devel webgrid/demo # Install the chart helm install k3s-guide-deployment webgrid/demo --version \"<pick-a-version-from-the-list>\" Gaining access \u00b6 For the simplicity of this guide, we will use a NodePort service to access the grid. To do so, create a file named webgrid-nodeport.yml with the following content: apiVersion : v1 kind : Service metadata : name : guide-webgrid-nodeport spec : type : NodePort ports : - port : 80 targetPort : http nodePort : 30007 protocol : TCP name : http selector : web-grid/component : proxy app.kubernetes.io/name : webgrid app.kubernetes.io/instance : k3s-guide-deployment You can now apply it by using the following command: kubectl apply -f webgrid-nodeport.yml Giving it a spin \u00b6 The grid is now fully operational and available on port 30007 of your cluster! You can either point your existing Selenium tests at it or use our testing tool by running the following command which runs three tests locally in Docker (you have to insert the clusters address): docker run --rm -it \\ -e ENDPOINT = http://<your-grid-endpoint>:30007 \\ -e FORKS = 3 \\ ghcr.io/tilblechschmidt/parallelseleniumtest:sha-fa30ad9 After experimenting a bit with it, you can visit your grid in a browser \u2014 just put the same URL in that you use for your Selenium clients \u2014 to view the screen recordings for your sessions!","title":"Deploying in K3s"},{"location":"guides/k3s/#deploying-in-k3s","text":"This guide will explain how to get a basic but feature-complete instance of WebGrid up and running in a cluster using k3s by Rancher . It will assume that you have a basic k3s cluster running and a working kubectl and helm locally available.","title":"Deploying in K3s"},{"location":"guides/k3s/#installing-with-helm","text":"To get started quickly, we will be deploying the demo chart. It contains the core grid along with external services that are required to get additional features like video recordings running. # Add the repository helm repo add webgrid https://webgrid.dev/ # List all available versions helm search repo --versions --devel webgrid/demo # Install the chart helm install k3s-guide-deployment webgrid/demo --version \"<pick-a-version-from-the-list>\"","title":"Installing with Helm"},{"location":"guides/k3s/#gaining-access","text":"For the simplicity of this guide, we will use a NodePort service to access the grid. To do so, create a file named webgrid-nodeport.yml with the following content: apiVersion : v1 kind : Service metadata : name : guide-webgrid-nodeport spec : type : NodePort ports : - port : 80 targetPort : http nodePort : 30007 protocol : TCP name : http selector : web-grid/component : proxy app.kubernetes.io/name : webgrid app.kubernetes.io/instance : k3s-guide-deployment You can now apply it by using the following command: kubectl apply -f webgrid-nodeport.yml","title":"Gaining access"},{"location":"guides/k3s/#giving-it-a-spin","text":"The grid is now fully operational and available on port 30007 of your cluster! You can either point your existing Selenium tests at it or use our testing tool by running the following command which runs three tests locally in Docker (you have to insert the clusters address): docker run --rm -it \\ -e ENDPOINT = http://<your-grid-endpoint>:30007 \\ -e FORKS = 3 \\ ghcr.io/tilblechschmidt/parallelseleniumtest:sha-fa30ad9 After experimenting a bit with it, you can visit your grid in a browser \u2014 just put the same URL in that you use for your Selenium clients \u2014 to view the screen recordings for your sessions!","title":"Giving it a spin"},{"location":"kubernetes/access/","text":"Accessing the grid \u00b6 By default the chart does not create an Ingress object for the Service. This means that the grid will only be accessible from within your cluster. You can use any Kubernetes mechanic to expose it to your clients \u2014 below are some examples to point you in the right direction. Note If you changed the name or namespace of the release during installation you have to adjust it accordingly in the examples below! Cluster internal access \u00b6 If your Selenium clients (e.g. test suites) are running inside your cluster you can directly interact with the Service created by the helm chart. In the default Kubernetes setup you can access it through either http://example-webgrid within the same namespace or http://example-webgrid.namespace from a different one. Port forwarding \u00b6 In case you have a local client and want to use the grid you can temporarily forward a local port to the Service in the cluster. Below is an example that would expose the grid at http://localhost:3030 . kubectl port-forward service/example-webgrid 3030:80 Ingress object \u00b6 If you want to access the grid from outside the cluster you can use any method Kubernetes provides to expose the Service. Below is an example configuration object. apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : webgrid spec : rules : - host : webgrid.your-host.dev http : paths : - path : / backend : serviceName : example-webgrid servicePort : http Note You may have to add additional properties to the spec for it to work depending on your cluster setup. Consult with your cluster admin (or documentation) for more details. Warning As the Ingress adds another HTTP proxy to the request chain it could pose a bottleneck! If you want to avoid this consider running your tests inside the cluster .","title":"Accessing the grid"},{"location":"kubernetes/access/#accessing-the-grid","text":"By default the chart does not create an Ingress object for the Service. This means that the grid will only be accessible from within your cluster. You can use any Kubernetes mechanic to expose it to your clients \u2014 below are some examples to point you in the right direction. Note If you changed the name or namespace of the release during installation you have to adjust it accordingly in the examples below!","title":"Accessing the grid"},{"location":"kubernetes/access/#cluster-internal-access","text":"If your Selenium clients (e.g. test suites) are running inside your cluster you can directly interact with the Service created by the helm chart. In the default Kubernetes setup you can access it through either http://example-webgrid within the same namespace or http://example-webgrid.namespace from a different one.","title":"Cluster internal access"},{"location":"kubernetes/access/#port-forwarding","text":"In case you have a local client and want to use the grid you can temporarily forward a local port to the Service in the cluster. Below is an example that would expose the grid at http://localhost:3030 . kubectl port-forward service/example-webgrid 3030:80","title":"Port forwarding"},{"location":"kubernetes/access/#ingress-object","text":"If you want to access the grid from outside the cluster you can use any method Kubernetes provides to expose the Service. Below is an example configuration object. apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : webgrid spec : rules : - host : webgrid.your-host.dev http : paths : - path : / backend : serviceName : example-webgrid servicePort : http Note You may have to add additional properties to the spec for it to work depending on your cluster setup. Consult with your cluster admin (or documentation) for more details. Warning As the Ingress adds another HTTP proxy to the request chain it could pose a bottleneck! If you want to avoid this consider running your tests inside the cluster .","title":"Ingress object"},{"location":"kubernetes/configuration/","text":"Configuration \u00b6 To get you started as quickly as possible the helm chart uses a set of default values that work in most clusters. However, these defaults are only an entrypoint \u2014 it is very likely that these need to be adapted to your specific needs. The Helm repository contains two charts: webgrid and demo . The latter wraps the former and provides some additional tools like an instance of Minio for storing video recordings and a web UI to access the database. Other documentation topics may ask you to change values in order to enable advanced features like screen recordings. Refer the sections below to learn how to do so. If you are using the demo Chart and want to change values, put them under the webgrid: key so they are forwarded to the webgrid Chart! Changing the defaults \u00b6 To override values you should create a file called webgrid-chart-values.yaml which contains the settings you want to change and apply it using helm: # During installation helm install -f webgrid-chart-values.yaml example webgrid/webgrid # Change a running grid helm upgrade -f webgrid-chart-values.yaml example webgrid/webgrid Note If you changed the name or namespace of the release during installation you have to adjust it accordingly in the example commands above! Value reference \u00b6 Below is a reference of all default helm values with their documentations. You can also find those in the source code of the chart. Default helm values # Default values for web-grid. # This is a YAML-formatted file. # Declare variables to be passed into your templates. logLevel : info,hyper=warn,warp=warn,sqlx=warn,tower=warn,h2=warn service : type : ClusterIP port : 80 config : # When set, sessions will be provided with this backend enabling video recording. # Currently, only S3 compatible storage backends are supported. The URL encapsulates all parameters needed to # connect to the backend. It follows the following pattern where values in brackets are optional: # s3+http[s]://key:secret@endpoint/bucket[?pathStyle] storageBackend : redis : # If you have your own Redis instance somewhere, you can point to it here. # When left empty, the chart will deploy its own one and use it. customEndpoint : # Storage class to use for persisting data when customEndpoint is not set storageClassName : emptyDir mongo : # If you have your own MongoDB instance somewhere, you can point to it here. # When left empty, the chart will deploy its own one and use it. customEndpoint : # Storage class to use for persisting data when customEndpoint is not set storageClassName : emptyDir storageSize : 16G manager : requiredMetadata : \"\" gangway : # Maximum number of cached service endpoints cacheSize : 1000 # Maximum number of concurrent, pending session creation requests. # When more requests arrive, the oldest ones will be terminated. # In reality, this variable is only here to cap the memory usage and not to actively control the requests. # When you are hitting this limit you should probably start scaling horizontally instead. pendingRequestLimit : 25000 collector : # Name of the mongo database to use database : webgrid # Capped collection to use for \"permanent\" storage collection : sessions # Byte limit for the collection (quoted because Helm likes its scientific notation ...) sizeLimit : \"17179869184\" # Settings for an intermediary \"staging\" collection # Since documents in capped collections are immutable, sessions are stored here temporarily staging : # Name of the collection collection : sessionsStaging # TTL for documents in the collection # This ensures that \"orphaned\" metadata gets cleaned up eventually. This can happen when # a session dies without sending a termination notification (e.g. during server failure) ttl : 3600 orchestrator : # Number of concurrent sessions allowed *per* orchestrator replica permits : 5 node : # Maximum duration (in seconds) the webdriver may take until it reports a ready state. startupTimeout : 120 # Idle timeout (in seconds) which is in effect until the first client request is received. # This allows the session to terminate early if the client no longer has any interest in the session # or it itself ran into a local timeout (e.g. due to prolonged queueing). # After the first request from a client has been received, the regular idle-timeout is taking effect. initialTimeout : 30 # If no WebDriver client request is received within the specified period (in seconds), the node will terminate. # Each incoming request resets the countdown. idleTimeout : 120 # Screen resolution for sessions resolution : 1920x1080 # Options related to video recording # Consult the ffmpeg documentation on how these parameters work # https://trac.ffmpeg.org/wiki/Encode/H.264 recording : crf : 46 maxBitrate : 450000 framerate : 5 segmentDuration : 6 replicaCount : gangway : 1 manager : 1 collector : 1 orchestrator : 1 image : repository : webgrid # Defaults to Chart.AppVersion if not set tag : \"\" pullPolicy : Always pullSecret : \"\" serviceAccount : # Specifies whether a service account with RBAC should be created create : true # Annotations to add to the service account annotations : {} # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name : \"\" resources : api : {} redis : {} mongo : {} gangway : {} manager : {} collector : {} orchestrator : {} node : limits : memory : 8000Mi requests : cpu : \"1\" memory : 4000Mi nodeSelector : api : {} redis : {} mongo : {} gangway : {} manager : {} collector : {} orchestrator : {} node : {} tolerations : api : [] redis : [] mongo : [] gangway : [] manager : [] collector : [] orchestrator : [] node : [] affinity : api : {} redis : {} mongo : {} gangway : {} manager : {} collector : {} orchestrator : {} node : {}","title":"Configuration"},{"location":"kubernetes/configuration/#configuration","text":"To get you started as quickly as possible the helm chart uses a set of default values that work in most clusters. However, these defaults are only an entrypoint \u2014 it is very likely that these need to be adapted to your specific needs. The Helm repository contains two charts: webgrid and demo . The latter wraps the former and provides some additional tools like an instance of Minio for storing video recordings and a web UI to access the database. Other documentation topics may ask you to change values in order to enable advanced features like screen recordings. Refer the sections below to learn how to do so. If you are using the demo Chart and want to change values, put them under the webgrid: key so they are forwarded to the webgrid Chart!","title":"Configuration"},{"location":"kubernetes/configuration/#changing-the-defaults","text":"To override values you should create a file called webgrid-chart-values.yaml which contains the settings you want to change and apply it using helm: # During installation helm install -f webgrid-chart-values.yaml example webgrid/webgrid # Change a running grid helm upgrade -f webgrid-chart-values.yaml example webgrid/webgrid Note If you changed the name or namespace of the release during installation you have to adjust it accordingly in the example commands above!","title":"Changing the defaults"},{"location":"kubernetes/configuration/#value-reference","text":"Below is a reference of all default helm values with their documentations. You can also find those in the source code of the chart. Default helm values # Default values for web-grid. # This is a YAML-formatted file. # Declare variables to be passed into your templates. logLevel : info,hyper=warn,warp=warn,sqlx=warn,tower=warn,h2=warn service : type : ClusterIP port : 80 config : # When set, sessions will be provided with this backend enabling video recording. # Currently, only S3 compatible storage backends are supported. The URL encapsulates all parameters needed to # connect to the backend. It follows the following pattern where values in brackets are optional: # s3+http[s]://key:secret@endpoint/bucket[?pathStyle] storageBackend : redis : # If you have your own Redis instance somewhere, you can point to it here. # When left empty, the chart will deploy its own one and use it. customEndpoint : # Storage class to use for persisting data when customEndpoint is not set storageClassName : emptyDir mongo : # If you have your own MongoDB instance somewhere, you can point to it here. # When left empty, the chart will deploy its own one and use it. customEndpoint : # Storage class to use for persisting data when customEndpoint is not set storageClassName : emptyDir storageSize : 16G manager : requiredMetadata : \"\" gangway : # Maximum number of cached service endpoints cacheSize : 1000 # Maximum number of concurrent, pending session creation requests. # When more requests arrive, the oldest ones will be terminated. # In reality, this variable is only here to cap the memory usage and not to actively control the requests. # When you are hitting this limit you should probably start scaling horizontally instead. pendingRequestLimit : 25000 collector : # Name of the mongo database to use database : webgrid # Capped collection to use for \"permanent\" storage collection : sessions # Byte limit for the collection (quoted because Helm likes its scientific notation ...) sizeLimit : \"17179869184\" # Settings for an intermediary \"staging\" collection # Since documents in capped collections are immutable, sessions are stored here temporarily staging : # Name of the collection collection : sessionsStaging # TTL for documents in the collection # This ensures that \"orphaned\" metadata gets cleaned up eventually. This can happen when # a session dies without sending a termination notification (e.g. during server failure) ttl : 3600 orchestrator : # Number of concurrent sessions allowed *per* orchestrator replica permits : 5 node : # Maximum duration (in seconds) the webdriver may take until it reports a ready state. startupTimeout : 120 # Idle timeout (in seconds) which is in effect until the first client request is received. # This allows the session to terminate early if the client no longer has any interest in the session # or it itself ran into a local timeout (e.g. due to prolonged queueing). # After the first request from a client has been received, the regular idle-timeout is taking effect. initialTimeout : 30 # If no WebDriver client request is received within the specified period (in seconds), the node will terminate. # Each incoming request resets the countdown. idleTimeout : 120 # Screen resolution for sessions resolution : 1920x1080 # Options related to video recording # Consult the ffmpeg documentation on how these parameters work # https://trac.ffmpeg.org/wiki/Encode/H.264 recording : crf : 46 maxBitrate : 450000 framerate : 5 segmentDuration : 6 replicaCount : gangway : 1 manager : 1 collector : 1 orchestrator : 1 image : repository : webgrid # Defaults to Chart.AppVersion if not set tag : \"\" pullPolicy : Always pullSecret : \"\" serviceAccount : # Specifies whether a service account with RBAC should be created create : true # Annotations to add to the service account annotations : {} # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name : \"\" resources : api : {} redis : {} mongo : {} gangway : {} manager : {} collector : {} orchestrator : {} node : limits : memory : 8000Mi requests : cpu : \"1\" memory : 4000Mi nodeSelector : api : {} redis : {} mongo : {} gangway : {} manager : {} collector : {} orchestrator : {} node : {} tolerations : api : [] redis : [] mongo : [] gangway : [] manager : [] collector : [] orchestrator : [] node : [] affinity : api : {} redis : {} mongo : {} gangway : {} manager : {} collector : {} orchestrator : {} node : {}","title":"Value reference"},{"location":"kubernetes/scaling/","text":"Scaling \u00b6 As the grid is optimized for performance, it should work for most small-scale scenarios. However, if you are running into performance issues it can be scaled up to meet demand. The grid is split into multiple distinct components and each can be replicated individually (apart from the database \u2014 but it should outperform everything else by a long stretch ). You should look at which component requires scaling. Below is a list of common scenarios where bottlenecks are expected. Concurrent sessions \u00b6 By default, the number of concurrent sessions is limited to five per orchestrator and one Kubernetes orchestrator. Normally, only one orchestrator is required even for very large setups so the per-orchestrator limit should be used. To change the number of concurrent sessions allowed (in this case 5 ) merge and apply the following helm value as described here : config : orchestrator : permits : 5 Note Make sure that your K8s service account has a sufficient quota available to create the required number of jobs and pods for the sessions. Traffic congestion \u00b6 Another common bottleneck, which is especially common with regular Selenium Grids, is the proxy server. Due to protocol constraints all traffic has to be routed through an intermediate instance, which inherently creates a choke point. This can be remedied by the microservice architecture of WebGrid. To increase the number of proxy servers that route session control traffic, merge and apply the following helm value as described here : replicaCount : proxy : 2","title":"Scaling"},{"location":"kubernetes/scaling/#scaling","text":"As the grid is optimized for performance, it should work for most small-scale scenarios. However, if you are running into performance issues it can be scaled up to meet demand. The grid is split into multiple distinct components and each can be replicated individually (apart from the database \u2014 but it should outperform everything else by a long stretch ). You should look at which component requires scaling. Below is a list of common scenarios where bottlenecks are expected.","title":"Scaling"},{"location":"kubernetes/scaling/#concurrent-sessions","text":"By default, the number of concurrent sessions is limited to five per orchestrator and one Kubernetes orchestrator. Normally, only one orchestrator is required even for very large setups so the per-orchestrator limit should be used. To change the number of concurrent sessions allowed (in this case 5 ) merge and apply the following helm value as described here : config : orchestrator : permits : 5 Note Make sure that your K8s service account has a sufficient quota available to create the required number of jobs and pods for the sessions.","title":"Concurrent sessions"},{"location":"kubernetes/scaling/#traffic-congestion","text":"Another common bottleneck, which is especially common with regular Selenium Grids, is the proxy server. Due to protocol constraints all traffic has to be routed through an intermediate instance, which inherently creates a choke point. This can be remedied by the microservice architecture of WebGrid. To increase the number of proxy servers that route session control traffic, merge and apply the following helm value as described here : replicaCount : proxy : 2","title":"Traffic congestion"},{"location":"kubernetes/storage/","text":"Grid storage \u00b6 Some features, like screen recording, require a persistent storage. Due to the vast possibilities to manage storage, no standard values are given and it is disabled by default. To enable it, merge and apply the following helm values as described here : config : storageBackend : <your-storage-url> S3 compatible \u00b6 Currently, only S3 compatible storage providers are supported. You can use them by providing a URl that follows this schema: s3+http(s)://user:pass@storageHost/bucket(?pathStyle) Here is an example for usage with a Minio instance hosted on webgrid-demo-storage with user webgrid , password supersecret , and bucket webgrid-video . Note that Minio uses path-style bucket URLs by default instead of subdomain style ones, so the storage URL contains the corresponding suffix. s3+http://webgrid:supersecret@webgrid-demo-storage/webgrid-video?pathStyle","title":"Grid storage"},{"location":"kubernetes/storage/#grid-storage","text":"Some features, like screen recording, require a persistent storage. Due to the vast possibilities to manage storage, no standard values are given and it is disabled by default. To enable it, merge and apply the following helm values as described here : config : storageBackend : <your-storage-url>","title":"Grid storage"},{"location":"kubernetes/storage/#s3-compatible","text":"Currently, only S3 compatible storage providers are supported. You can use them by providing a URl that follows this schema: s3+http(s)://user:pass@storageHost/bucket(?pathStyle) Here is an example for usage with a Minio instance hosted on webgrid-demo-storage with user webgrid , password supersecret , and bucket webgrid-video . Note that Minio uses path-style bucket URLs by default instead of subdomain style ones, so the storage URL contains the corresponding suffix. s3+http://webgrid:supersecret@webgrid-demo-storage/webgrid-video?pathStyle","title":"S3 compatible"}]}